{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主流程文件 Main Training Script\n",
    "\n",
    "> 主训练脚本入口，调用各模块进行模型训练\n",
    "> \n",
    "> The main entry point for running training, orchestrating all modules for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介/Description:\n",
    "main 模块是项目的主训练入口。它结合了 core 模块中的任务定义和 data 模块中的数据加载功能，通过调用 PyTorch Lightning 的 Trainer 对模型进行训练。用户可以通过配置类快速切换不同的数据集、模型和训练策略，灵活完成实验任务。\n",
    "\n",
    "The main module serves as the primary entry point for training. It combines task definitions from the core module and data loading from the data module to execute model training via PyTorch Lightning’s Trainer. Users can flexibly switch between different datasets, models, and training strategies through configuration classes to perform experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要符号/Main symbols:\n",
    "\n",
    "- Trainer: PyTorch Lightning 的训练控制器，用于管理训练过程。  \n",
    "  \n",
    "  Trainer: The PyTorch Lightning controller for managing the training process.\n",
    "\n",
    "- ClassificationTask: 从 core 导入，用于模型训练的主要任务类。\n",
    "  \n",
    "  ClassificationTask: Imported from core, the primary task class for model training.\n",
    "\n",
    "- CIFAR100DataModule: 从 data 导入的数据加载模块。\n",
    "  \n",
    "  CIFAR100DataModule: Data loading module imported from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp __main__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "/home/ycm/program_files/managers/conda/envs/hf_ai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">cls_model </span><span style=\"color: #008000; text-decoration-color: #008000\">(HuggingfaceModel)</span>\n",
       "    ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">backbone </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTModel)</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">embeddings </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTEmbeddings) </span><span style=\"color: #008080; text-decoration-color: #008080\">cls_token:[1, 1, 768] position_embeddings:[1, 197, 768]</span>\n",
       "    │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">patch_embeddings </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTPatchEmbeddings)</span>\n",
       "    │   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">projection </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 3, 16, 16] bias:[768]</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">encoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTEncoder)</span>\n",
       "    │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "    │   │       └── <span style=\"color: #800000; text-decoration-color: #800000\">0-11</span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTLayer)</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">attention </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTAttention)</span>\n",
       "    │   │           │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">attention </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTSelfAttention)</span>\n",
       "    │   │           │   │   └── <span style=\"color: #800000; text-decoration-color: #800000\">query,key,value</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 768] bias:[768]</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">output </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTSelfOutput)</span>\n",
       "    │   │           │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 768] bias:[768]</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">intermediate </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTIntermediate)</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[3072, 768] bias:[3072]</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">output </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTOutput)</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 3072] bias:[768]</span>\n",
       "    │   │           └── <span style=\"color: #800000; text-decoration-color: #800000\">layernorm_before,layernorm_after</span><span style=\"color: #008000; text-decoration-color: #008000\">(LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768] bias:[768]</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layernorm </span><span style=\"color: #008000; text-decoration-color: #008000\">(LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768] bias:[768]</span>\n",
       "    │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">pooler </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTPooler)</span>\n",
       "    │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 768] bias:[768]</span>\n",
       "    └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">head </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[100, 768] bias:[100]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "└── \u001b[37mcls_model \u001b[0m\u001b[32m(HuggingfaceModel)\u001b[0m\n",
       "    ├── \u001b[37mbackbone \u001b[0m\u001b[32m(ViTModel)\u001b[0m\n",
       "    │   ├── \u001b[37membeddings \u001b[0m\u001b[32m(ViTEmbeddings) \u001b[0m\u001b[36mcls_token:[1, 1, 768] \u001b[0m\u001b[36mposition_embeddings:[1, 197, 768]\u001b[0m\n",
       "    │   │   └── \u001b[37mpatch_embeddings \u001b[0m\u001b[32m(ViTPatchEmbeddings)\u001b[0m\n",
       "    │   │       └── \u001b[37mprojection \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[768, 3, 16, 16] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   ├── \u001b[37mencoder \u001b[0m\u001b[32m(ViTEncoder)\u001b[0m\n",
       "    │   │   └── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "    │   │       └── \u001b[31m0-11\u001b[0m\u001b[32m(ViTLayer)\u001b[0m\n",
       "    │   │           ├── \u001b[37mattention \u001b[0m\u001b[32m(ViTAttention)\u001b[0m\n",
       "    │   │           │   ├── \u001b[37mattention \u001b[0m\u001b[32m(ViTSelfAttention)\u001b[0m\n",
       "    │   │           │   │   └── \u001b[31mquery,key,value\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   │           │   └── \u001b[37moutput \u001b[0m\u001b[32m(ViTSelfOutput)\u001b[0m\n",
       "    │   │           │       └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   │           ├── \u001b[37mintermediate \u001b[0m\u001b[32m(ViTIntermediate)\u001b[0m\n",
       "    │   │           │   └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[3072, 768] \u001b[0m\u001b[36mbias:[3072]\u001b[0m\n",
       "    │   │           ├── \u001b[37moutput \u001b[0m\u001b[32m(ViTOutput)\u001b[0m\n",
       "    │   │           │   └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 3072] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   │           └── \u001b[31mlayernorm_before,layernorm_after\u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[36mweight:[768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   ├── \u001b[37mlayernorm \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[36mweight:[768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   └── \u001b[37mpooler \u001b[0m\u001b[32m(ViTPooler)\u001b[0m\n",
       "    │       └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    └── \u001b[37mhead \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[100, 768] \u001b[0m\u001b[36mbias:[100]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "from namable_classify.core import ClassificationTask, ClassificationTaskConfig\n",
    "config = ClassificationTaskConfig()\n",
    "# config.learning_rate = 1e-1\n",
    "# config.learning_rate = 1\n",
    "# config.learning_rate = 1e-3\n",
    "# config.learning_rate = 1e-5\n",
    "config.learning_rate = 3e-4\n",
    "config.experiment_index = 1\n",
    "# config.learning_rate = 1e-6\n",
    "config.dataset_config.batch_size = 64\n",
    "cls_task = ClassificationTask(config)\n",
    "cls_task.print_model_pretty()\n",
    "import torch\n",
    "# cls_task.cls_model = torch.compile(cls_task.cls_model, mode='reduce-overhead')\n",
    "#  fullgraph=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 约取 (YueQu) , the model structure is: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">cls_model </span><span style=\"color: #008000; text-decoration-color: #008000\">(HuggingfaceModel)</span>\n",
       "    ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">backbone </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTModel)</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">embeddings </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTEmbeddings) </span><span style=\"color: #008080; text-decoration-color: #008080\">cls_token:[1, 1, 768] position_embeddings:[1, 197, 768]</span>\n",
       "    │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">patch_embeddings </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTPatchEmbeddings)</span>\n",
       "    │   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">projection </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 3, 16, 16] bias:[768]</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">encoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTEncoder)</span>\n",
       "    │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "    │   │       └── <span style=\"color: #800000; text-decoration-color: #800000\">0-11</span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTLayer)</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">attention </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTAttention)</span>\n",
       "    │   │           │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">attention </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTSelfAttention)</span>\n",
       "    │   │           │   │   └── <span style=\"color: #800000; text-decoration-color: #800000\">query,key,value</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 768] bias:[768]</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">output </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTSelfOutput)</span>\n",
       "    │   │           │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 768] bias:[768]</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">intermediate </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTIntermediate)</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[3072, 768] bias:[3072]</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">output </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTOutput)</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 3072] bias:[768]</span>\n",
       "    │   │           └── <span style=\"color: #800000; text-decoration-color: #800000\">layernorm_before,layernorm_after</span><span style=\"color: #008000; text-decoration-color: #008000\">(LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768] bias:[768]</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layernorm </span><span style=\"color: #008000; text-decoration-color: #008000\">(LayerNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768] bias:[768]</span>\n",
       "    │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">pooler </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTPooler)</span>\n",
       "    │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 768] bias:[768]</span>\n",
       "    └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">head </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[100, 768] bias:[100]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "└── \u001b[37mcls_model \u001b[0m\u001b[32m(HuggingfaceModel)\u001b[0m\n",
       "    ├── \u001b[37mbackbone \u001b[0m\u001b[32m(ViTModel)\u001b[0m\n",
       "    │   ├── \u001b[37membeddings \u001b[0m\u001b[32m(ViTEmbeddings) \u001b[0m\u001b[36mcls_token:[1, 1, 768] \u001b[0m\u001b[36mposition_embeddings:[1, 197, 768]\u001b[0m\n",
       "    │   │   └── \u001b[37mpatch_embeddings \u001b[0m\u001b[32m(ViTPatchEmbeddings)\u001b[0m\n",
       "    │   │       └── \u001b[37mprojection \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[768, 3, 16, 16] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   ├── \u001b[37mencoder \u001b[0m\u001b[32m(ViTEncoder)\u001b[0m\n",
       "    │   │   └── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "    │   │       └── \u001b[31m0-11\u001b[0m\u001b[32m(ViTLayer)\u001b[0m\n",
       "    │   │           ├── \u001b[37mattention \u001b[0m\u001b[32m(ViTAttention)\u001b[0m\n",
       "    │   │           │   ├── \u001b[37mattention \u001b[0m\u001b[32m(ViTSelfAttention)\u001b[0m\n",
       "    │   │           │   │   └── \u001b[31mquery,key,value\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   │           │   └── \u001b[37moutput \u001b[0m\u001b[32m(ViTSelfOutput)\u001b[0m\n",
       "    │   │           │       └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   │           ├── \u001b[37mintermediate \u001b[0m\u001b[32m(ViTIntermediate)\u001b[0m\n",
       "    │   │           │   └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[3072, 768] \u001b[0m\u001b[36mbias:[3072]\u001b[0m\n",
       "    │   │           ├── \u001b[37moutput \u001b[0m\u001b[32m(ViTOutput)\u001b[0m\n",
       "    │   │           │   └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 3072] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   │           └── \u001b[31mlayernorm_before,layernorm_after\u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[36mweight:[768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   ├── \u001b[37mlayernorm \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[36mweight:[768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    │   └── \u001b[37mpooler \u001b[0m\u001b[32m(ViTPooler)\u001b[0m\n",
       "    │       └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 768] \u001b[0m\u001b[36mbias:[768]\u001b[0m\n",
       "    └── \u001b[37mhead \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[100, 768] \u001b[0m\u001b[36mbias:[100]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LORA Algorithm from Hugging Face PEFT Library. \n",
      "peft_config: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules=['query', 'value'], lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))\n",
      "After 约取 (YueQu) , the model structure is: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">cls_model </span><span style=\"color: #008000; text-decoration-color: #008000\">(HuggingfaceModel)</span>\n",
       "    ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">backbone </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTModel)</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">embeddings </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTEmbeddings) </span><span style=\"color: #004664; text-decoration-color: #004664\">cls_token:[1, 1, 768] position_embeddings:[1, 197, 768]</span>\n",
       "    │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">patch_embeddings </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTPatchEmbeddings)</span>\n",
       "    │   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">projection </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768, 3, 16, 16] bias:[768]</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">encoder </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTEncoder)</span>\n",
       "    │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "    │   │       └── <span style=\"color: #800000; text-decoration-color: #800000\">0-11</span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTLayer)</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">attention </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTAttention)</span>\n",
       "    │   │           │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">attention </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTSelfAttention)</span>\n",
       "    │   │           │   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">query,value</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear)</span>\n",
       "    │   │           │   │   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">base_layer </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768, 768] bias:[768]</span>\n",
       "    │   │           │   │   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">lora_dropout </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleDict)</span>\n",
       "    │   │           │   │   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">lora_A </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleDict)</span>\n",
       "    │   │           │   │   │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">default </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[8, 768]</span>\n",
       "    │   │           │   │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">lora_B </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleDict)</span>\n",
       "    │   │           │   │   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">default </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[768, 8]</span>\n",
       "    │   │           │   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">key </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768, 768] bias:[768]</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">output </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTSelfOutput)</span>\n",
       "    │   │           │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768, 768] bias:[768]</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">intermediate </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTIntermediate)</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[3072, 768] bias:[3072]</span>\n",
       "    │   │           ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">output </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTOutput)</span>\n",
       "    │   │           │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768, 3072] bias:[768]</span>\n",
       "    │   │           └── <span style=\"color: #800000; text-decoration-color: #800000\">layernorm_before,layernorm_after</span><span style=\"color: #008000; text-decoration-color: #008000\">(LayerNorm) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768] bias:[768]</span>\n",
       "    │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layernorm </span><span style=\"color: #008000; text-decoration-color: #008000\">(LayerNorm) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768] bias:[768]</span>\n",
       "    │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">pooler </span><span style=\"color: #008000; text-decoration-color: #008000\">(ViTPooler)</span>\n",
       "    │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dense </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[768, 768] bias:[768]</span>\n",
       "    └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">head </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #004664; text-decoration-color: #004664\">weight:[100, 768] bias:[100]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "└── \u001b[37mcls_model \u001b[0m\u001b[32m(HuggingfaceModel)\u001b[0m\n",
       "    ├── \u001b[37mbackbone \u001b[0m\u001b[32m(ViTModel)\u001b[0m\n",
       "    │   ├── \u001b[37membeddings \u001b[0m\u001b[32m(ViTEmbeddings) \u001b[0m\u001b[38;2;0;70;100mcls_token:[1, 1, 768] \u001b[0m\u001b[38;2;0;70;100mposition_embeddings:[1, 197, 768]\u001b[0m\n",
       "    │   │   └── \u001b[37mpatch_embeddings \u001b[0m\u001b[32m(ViTPatchEmbeddings)\u001b[0m\n",
       "    │   │       └── \u001b[37mprojection \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[38;2;0;70;100mweight:[768, 3, 16, 16] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    │   ├── \u001b[37mencoder \u001b[0m\u001b[32m(ViTEncoder)\u001b[0m\n",
       "    │   │   └── \u001b[37mlayer \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "    │   │       └── \u001b[31m0-11\u001b[0m\u001b[32m(ViTLayer)\u001b[0m\n",
       "    │   │           ├── \u001b[37mattention \u001b[0m\u001b[32m(ViTAttention)\u001b[0m\n",
       "    │   │           │   ├── \u001b[37mattention \u001b[0m\u001b[32m(ViTSelfAttention)\u001b[0m\n",
       "    │   │           │   │   ├── \u001b[31mquery,value\u001b[0m\u001b[32m(Linear)\u001b[0m\n",
       "    │   │           │   │   │   ├── \u001b[37mbase_layer \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[38;2;0;70;100mweight:[768, 768] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    │   │           │   │   │   ├── \u001b[37mlora_dropout \u001b[0m\u001b[32m(ModuleDict)\u001b[0m\n",
       "    │   │           │   │   │   ├── \u001b[37mlora_A \u001b[0m\u001b[32m(ModuleDict)\u001b[0m\n",
       "    │   │           │   │   │   │   └── \u001b[37mdefault \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[8, 768]\u001b[0m\n",
       "    │   │           │   │   │   └── \u001b[37mlora_B \u001b[0m\u001b[32m(ModuleDict)\u001b[0m\n",
       "    │   │           │   │   │       └── \u001b[37mdefault \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[768, 8]\u001b[0m\n",
       "    │   │           │   │   └── \u001b[37mkey \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[38;2;0;70;100mweight:[768, 768] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    │   │           │   └── \u001b[37moutput \u001b[0m\u001b[32m(ViTSelfOutput)\u001b[0m\n",
       "    │   │           │       └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[38;2;0;70;100mweight:[768, 768] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    │   │           ├── \u001b[37mintermediate \u001b[0m\u001b[32m(ViTIntermediate)\u001b[0m\n",
       "    │   │           │   └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[38;2;0;70;100mweight:[3072, 768] \u001b[0m\u001b[38;2;0;70;100mbias:[3072]\u001b[0m\n",
       "    │   │           ├── \u001b[37moutput \u001b[0m\u001b[32m(ViTOutput)\u001b[0m\n",
       "    │   │           │   └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[38;2;0;70;100mweight:[768, 3072] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    │   │           └── \u001b[31mlayernorm_before,layernorm_after\u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[38;2;0;70;100mweight:[768] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    │   ├── \u001b[37mlayernorm \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[38;2;0;70;100mweight:[768] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    │   └── \u001b[37mpooler \u001b[0m\u001b[32m(ViTPooler)\u001b[0m\n",
       "    │       └── \u001b[37mdense \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[38;2;0;70;100mweight:[768, 768] \u001b[0m\u001b[38;2;0;70;100mbias:[768]\u001b[0m\n",
       "    └── \u001b[37mhead \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[38;2;0;70;100mweight:[100, 768] \u001b[0m\u001b[38;2;0;70;100mbias:[100]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<boguan_yuequ.auto.AutoYueQuAlgorithm at 0x71cb46476110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from boguan_yuequ.auto import AutoYueQuAlgorithm\n",
    "AutoYueQuAlgorithm(cls_task, \"LORA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# import lightning as L\n",
    "# trainer = L.Trainer()\n",
    "# from lightning.pytorch.tuner import Tuner\n",
    "# tuner = Tuner(trainer)\n",
    "# found_batch_size = tuner.scale_batch_size(cls_task, datamodule=cls_task.lit_data, \n",
    "#                                         #   mode='binsearch', \n",
    "#                                           mode='power', \n",
    "#                                           init_val=64)\n",
    "# # found_batch_size, cls_task.lit_data.hparams.batch_size\n",
    "# print(f\"Found batch size: {found_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import lightning as L\n",
    "from namable_classify.utils import runs_path\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger\n",
    "\n",
    "trainer = L.Trainer(default_root_dir=runs_path, enable_checkpointing=True, \n",
    "                    enable_model_summary=True, \n",
    "                    num_sanity_val_steps=2, # 防止 val 在训了好久train才发现崩溃\n",
    "                    callbacks=[\n",
    "                        # EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "                        EarlyStopping(monitor=\"val_acc1\", mode=\"max\", check_finite=True, \n",
    "                                      patience=5, \n",
    "                                    #   patience=6, \n",
    "                                      check_on_train_epoch_end=False,  # check on validation end\n",
    "                                      verbose=True),\n",
    "                        ModelSummary(max_depth=3),\n",
    "                        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n",
    "                        # StochasticWeightAveraging(swa_lrs=1e-2), \n",
    "                        # DeviceStatsMonitor(cpu_stats=True)\n",
    "                        LearningRateMonitor(), \n",
    "                        # LearningRateFinder() # 有奇怪的bug\n",
    "                               ]\n",
    "                    , max_epochs=15\n",
    "                    # , gradient_clip_val=1.0, gradient_clip_algorithm=\"value\"\n",
    "                    , logger=[\n",
    "                        # TensorBoardLogger(save_dir=runs_path/\"tensorboard\"),\n",
    "                        TensorBoardLogger(save_dir=runs_path),\n",
    "                              CSVLogger(save_dir=runs_path), \n",
    "                              WandbLogger(project=\"namable_classify\", name=\"test\")\n",
    "                              ]\n",
    "                    # , profiler=\"simple\"\n",
    "                    # , fast_dev_run=True\n",
    "                    # limit_train_batches=10, limit_val_batches=5\n",
    "                    # strategy=\"ddp\", accelerator=\"gpu\", devices=4\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from lightning.pytorch.tuner import Tuner\n",
    "# tuner = Tuner(trainer)\n",
    "\n",
    "# lr_finder = tuner.lr_find(cls_task, datamodule=cls_task.lit_data, \n",
    "#                         #   max_lr=1e-2\n",
    "#                         method = \"fit\",\n",
    "#                         min_lr = 1e-8,\n",
    "#     max_lr = 1,\n",
    "#     num_training = 100,\n",
    "#     mode = \"exponential\"\n",
    "                        \n",
    "#                           )\n",
    "# print(lr_finder.results)\n",
    "\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# from matplotlib import pyplot as plt\n",
    "# from namable_classify.utils import runs_figs_path\n",
    "# plt.savefig(runs_figs_path/'lr_finder.png')\n",
    "# # fig.show()\n",
    "# new_lr = lr_finder.suggestion()\n",
    "# # new_lr, cls_task.hparams.learning_rate\n",
    "# print(\"New learning rate: \", new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "trainer.fit(cls_task, datamodule=cls_task.lit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "trainer.test(cls_task, datamodule=cls_task.lit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2603119857\u001b[0m (\u001b[33mhandicraft-computing\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c68a7624ad54c878f11c5ddcea0777e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011136492067534063, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241017_232456-3erozzos</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/handicraft-computing/namable_classify/runs/3erozzos' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/handicraft-computing/namable_classify' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/handicraft-computing/namable_classify' target=\"_blank\">https://wandb.ai/handicraft-computing/namable_classify</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/handicraft-computing/namable_classify/runs/3erozzos' target=\"_blank\">https://wandb.ai/handicraft-computing/namable_classify/runs/3erozzos</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/ycm/repos/research/cv/cls/NamableClassify/deprecated/lightning_logs/version_53/checkpoints/epoch=11-step=8448.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loaded model weights from the checkpoint at /home/ycm/repos/research/cv/cls/NamableClassify/deprecated/lightning_logs/version_53/checkpoints/epoch=11-step=8448.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecea260a9a84368bf99a9ae0791210e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc1         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9221000075340271     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_acc10         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9961000084877014     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc2         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9702000021934509     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_acc20         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9980000257492065     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc3         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9850000143051147     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc5         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.991599977016449     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_balanced_accuracy   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9221000075340271     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_cohen_kappa      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9213131070137024     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9220957159996033     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_hinge_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2577705681324005     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_log_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4029653072357178     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0386210680007935     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_matthews_corrcoef   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9213470220565796     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9248200058937073     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9221000075340271     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_roc_auc        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9985058903694153     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc1        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9221000075340271    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_acc10        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9961000084877014    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc2        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9702000021934509    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_acc20        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9980000257492065    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc3        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9850000143051147    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc5        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.991599977016449    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_balanced_accuracy  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9221000075340271    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_cohen_kappa     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9213131070137024    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9220957159996033    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_hinge_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2577705681324005    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_log_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4029653072357178    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0386210680007935    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_matthews_corrcoef  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9213470220565796    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9248200058937073    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9221000075340271    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_roc_auc       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9985058903694153    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.0386210680007935,\n",
       "  'test_acc1': 0.9221000075340271,\n",
       "  'test_acc2': 0.9702000021934509,\n",
       "  'test_acc3': 0.9850000143051147,\n",
       "  'test_acc5': 0.991599977016449,\n",
       "  'test_acc10': 0.9961000084877014,\n",
       "  'test_acc20': 0.9980000257492065,\n",
       "  'test_roc_auc': 0.9985058903694153,\n",
       "  'test_matthews_corrcoef': 0.9213470220565796,\n",
       "  'test_f1': 0.9220957159996033,\n",
       "  'test_precision': 0.9248200058937073,\n",
       "  'test_recall': 0.9221000075340271,\n",
       "  'test_log_loss': 0.4029653072357178,\n",
       "  'test_balanced_accuracy': 0.9221000075340271,\n",
       "  'test_cohen_kappa': 0.9213131070137024,\n",
       "  'test_hinge_loss': 0.2577705681324005}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from namable_classify.utils import lib_repo_path\n",
    "trainer.test(cls_task, datamodule=cls_task.lit_data, \n",
    "             ckpt_path=lib_repo_path/\"deprecated/lightning_logs/version_53/checkpoints/epoch=11-step=8448.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
