{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载模块 Data Module\n",
    "\n",
    "<!-- > 我们支持多种数据集（如 CIFAR100、VTAB 等），将 数据加载 的逻辑拆分到单独的笔记本中，这样可以保持模块化，数据处理和训练逻辑分离，增强可维护性和扩展性。 -->\n",
    "> 处理数据加载和预处理的模块\n",
    "> \n",
    "> Handles data loading and preprocessing for datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介/Description:\n",
    "数据模块主要负责数据集的加载与预处理。DatasetConfig 使用 Pydantic 进行配置管理，以保证数据集参数的正确性，并通过 ClassificationDataModule 实现 PyTorch Lightning 的数据模块封装。此模块支持自定义数据转换，并为不同的数据集（如 CIFAR100）提供灵活的加载方案。\n",
    "\n",
    "The data module focuses on handling dataset loading and preprocessing. DatasetConfig is managed through Pydantic for configuration accuracy, and ClassificationDataModule encapsulates the PyTorch Lightning DataModule. This module supports custom data transforms and offers flexible loading schemes for various datasets such as CIFAR100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要符号/Main symbols:\n",
    "\n",
    "- DatasetConfig: Pydantic 定义的配置类，用于数据模块的参数管理。\n",
    "  \n",
    "  DatasetConfig: A Pydantic configuration class for managing data module parameters.\n",
    "\n",
    "- ClassificationDataModule: 用于 PyTorch Lightning 的数据模块封装，支持训练、验证、测试数据加载。\n",
    "  \n",
    "  ClassificationDataModule: A PyTorch Lightning DataModule wrapper supporting train, validation, and test data loading.\n",
    "  \n",
    "- CIFAR100DataModule: 基于 ClassificationDataModule 的具体数据模块实现，加载 CIFAR100 数据集。\n",
    "  \n",
    "  CIFAR100DataModule: A concrete implementation of ClassificationDataModule for loading the CIFAR100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from namable_classify.utils import data_path, Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ClassificationDataConfig(BaseModel):\n",
    "    # protocol: str = 'torch'\n",
    "    # dataset_name: str = 'cifar100'\n",
    "    dataset_root: Path = data_path\n",
    "    dataset_name: str = 'CIFAR100'\n",
    "    batch_size:int=1\n",
    "# TODO 支持多个来源的数据集自动加载\n",
    "# from torchvision.datasets import __all__, CIFAR100\n",
    "# __all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import lightning as L\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "\n",
    "\n",
    "class ClassificationDataModule(L.LightningDataModule):\n",
    "    num_of_classes = None\n",
    "    classes = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config:ClassificationDataConfig) -> 'ClassificationDataModule':\n",
    "        return cls(**config.model_dump())\n",
    "    def __init__(self, **config:ClassificationDataConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.save_hyperparameters(dict(num_of_classes=self.num_of_classes))\n",
    "        self.workers = 31 # TODO 根据CPU自动设置\n",
    "    #     self.__sub_init__()\n",
    "    #     # self.config = config\n",
    "    # def __sub_init__(self, a=1, b=2) -> None:\n",
    "    #     print(\"Hello\")\n",
    "    #     self.save_hyperparameters(dict(a=a, b=b))\n",
    "    # @property\n",
    "    # def num_classes(self) -> int:\n",
    "    #     return self.hparams.num_classes\n",
    "    # @num_classes.setter\n",
    "    # def num_classes(self, value:int) -> None:\n",
    "    #     self.hparams.num_classes = value\n",
    "        \n",
    "    # @property\n",
    "    # def transform(self) -> Callable: #TODO 类型标注不知道怎么写\n",
    "    #     return self.hparams.transform\n",
    "    \n",
    "    # @transform.setter\n",
    "    # def transform(self, value:Callable) -> None:\n",
    "    #     self.hparams.transform = value\n",
    "    \n",
    "    \n",
    "        \n",
    "    def train_dataloader(self)->TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_ds, batch_size=self.hparams.batch_size, \n",
    "                          num_workers=self.workers, shuffle=True, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self)->EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_ds, batch_size=self.hparams.batch_size, num_workers=self.workers, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self)->EVAL_DATALOADERS:\n",
    "        return DataLoader(self.test_ds, batch_size=self.hparams.batch_size, num_workers=self.workers, pin_memory=True)\n",
    "\n",
    "    def predict_dataloader(self)->EVAL_DATALOADERS:\n",
    "        return DataLoader(self.predict_ds, batch_size=self.hparams.batch_size, num_workers=self.workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"batch_size\":     1\n",
       "\"dataset_name\":   CIFAR100\n",
       "\"dataset_root\":   /home/ycm/repos/research/cv/cls/NamableClassify/data\n",
       "\"num_of_classes\": None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning as L\n",
    "L.seed_everything(42)\n",
    "cdm = ClassificationDataModule.from_config(ClassificationDataConfig())\n",
    "cdm.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "def sksplit_for_torch(ds_full:torch.utils.data.Dataset, test_size:float=0.2, stratify_targets=None, random_state=None):\n",
    "    indexes = list(range(len(ds_full)))\n",
    "    stratify_targets = stratify_targets or (ds_full.targets if hasattr(ds_full, 'targets') else [int(ds_full[i][1]) for i in indexes])\n",
    "    train_indexes, val_indexes = train_test_split(indexes, test_size=test_size,\n",
    "                                                    stratify=stratify_targets, random_state=random_state)\n",
    "    return torch.utils.data.Subset(ds_full, train_indexes), torch.utils.data.Subset(ds_full, val_indexes)\n",
    "    # return random_split(\n",
    "                #     ds_full, [, ], \n",
    "                #     generator=torch.Generator().manual_seed(L.seed_everything()), \n",
    "                # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_full = CIFAR100(data_path, train=True)\n",
    "train_ds, val_ds = sksplit_for_torch(ds_full, test_size=0.1)\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import lightning as L\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "# CIFAR100.url = # Tsinghua mirrorURL\n",
    "class TorchVisionDataModule(ClassificationDataModule):\n",
    "    torchvision_cls = CIFAR100\n",
    "    num_of_classes = 100\n",
    "    def __init__(self, \n",
    "                 train_transform=None, # 需要后续设置\n",
    "                 test_transform=None, # 需要后续设置\n",
    "                 train_val_split=0.9, # 训练集和验证集的比例\n",
    "                 **config:ClassificationDataConfig) -> None:\n",
    "        super().__init__(**config)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        train_ds = self.torchvision_cls(self.hparams.dataset_root, train=True, download=True)\n",
    "        test_ds = self.torchvision_cls(self.hparams.dataset_root, train=False, download=True)\n",
    "        self.classes = train_ds.classes\n",
    "        assert len(self.classes)==self.num_of_classes, f\"Number of classes in dataset is {len(self.classes)}, but {self.num_of_classes} is expected.\"\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        match (stage):\n",
    "            case (\"fit\"):\n",
    "                ds_full = self.torchvision_cls(self.hparams.dataset_root, train=True, transform=self.hparams.train_transform)\n",
    "                self.train_ds, self.val_ds = sksplit_for_torch(ds_full, 1-self.hparams.train_val_split, random_state=0) # 不和Lighning seed everything一起\n",
    "                # 还有个 validate 但是fit的时候我们就设置好了，所以直接跳过\n",
    "            case (\"validate\"):\n",
    "                print(\"Validation loader has been setup before. \")\n",
    "                pass\n",
    "            case (\"test\"):\n",
    "                self.test_ds = self.torchvision_cls(self.hparams.dataset_root, train=False, transform=self.hparams.test_transform)\n",
    "            case (\"predict\"):\n",
    "                self.predict_ds = self.torchvision_cls(self.hparams.dataset_root, train=False, transform=self.hparams.test_transform)\n",
    "\n",
    "class CIFAR100DataModule(TorchVisionDataModule):\n",
    "    torchvision_cls = CIFAR100\n",
    "    num_of_classes = 100\n",
    "    \n",
    "class MNISTDataModule(TorchVisionDataModule):\n",
    "    torchvision_cls = MNIST\n",
    "    num_of_classes = 10\n",
    "    \n",
    "class CIFAR10DataModule(TorchVisionDataModule):\n",
    "    torchvision_cls = CIFAR10\n",
    "    num_of_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"batch_size\":      1\n",
       "\"dataset_name\":    MNIST\n",
       "\"dataset_root\":    /home/ycm/repos/research/cv/cls/NamableClassify/data\n",
       "\"num_of_classes\":  10\n",
       "\"test_transform\":  None\n",
       "\"train_transform\": None\n",
       "\"train_val_split\": 0.9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = MNISTDataModule.from_config(ClassificationDataConfig(dataset_name='MNIST'))\n",
    "mnist_data.prepare_data()\n",
    "mnist_data.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"batch_size\":      1\n",
       "\"dataset_name\":    CIFAR100\n",
       "\"dataset_root\":    /home/ycm/repos/research/cv/cls/NamableClassify/data\n",
       "\"num_of_classes\":  100\n",
       "\"test_transform\":  None\n",
       "\"train_transform\": None\n",
       "\"train_val_split\": 0.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar100_data = CIFAR100DataModule.from_config(ClassificationDataConfig(dataset_name=\"CIFAR100\"))\n",
    "cifar100_data.prepare_data()\n",
    "cifar100_data.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ycm/program_files/managers/conda/envs/hf_ai/lib/python3.10/site-packages/torchvision/datasets/cifar.py'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getfile(CIFAR100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO VTAB dataset\n",
    "# vtab_dir: str = \"/home/ai_pitch_perfector/datasets/vtab-1k/\"\n",
    "# subset_name: str = \"cifar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import AutoImageProcessor, BitImageProcessor, ViTImageProcessor\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    RandomRotation,\n",
    "    RandomGrayscale,\n",
    "    Grayscale,\n",
    "    AutoAugment,\n",
    "    RandAugment,\n",
    ")\n",
    "from namable_classify.data.transforms import CutoutPIL\n",
    "from fastcore.basics import patch\n",
    "@patch\n",
    "def set_transform_from_hf_image_preprocessor(self:ClassificationDataModule, hf_image_preprocessor:AutoImageProcessor, model_image_size=None):\n",
    "    if model_image_size is None:\n",
    "        if isinstance(hf_image_preprocessor, ViTImageProcessor):\n",
    "            model_image_size:tuple[int, int] = (hf_image_preprocessor.size['height'], hf_image_preprocessor.size['width'])\n",
    "        elif isinstance(hf_image_preprocessor, BitImageProcessor):\n",
    "            model_image_size:tuple[int, int] = (hf_image_preprocessor.crop_size['height'], hf_image_preprocessor.crop_size['width'])\n",
    "    normalize = Normalize(mean=hf_image_preprocessor.image_mean, std=hf_image_preprocessor.image_std)\n",
    "    self.hparams.train_transform = Compose(\n",
    "        [\n",
    "            # # RandomResizedCrop(image_processor.size[\"height\"]),\n",
    "            # RandomResizedCrop(image_processor.crop_size[\"height\"]),\n",
    "            # RandomHorizontalFlip(),\n",
    "            # # RandomRotation((-30, 30)),\n",
    "            # # RandomGrayscale(),\n",
    "            # # AddPepperNoise(0.5, p=0.1),\n",
    "            # Grayscale(num_output_channels=3),\n",
    "\n",
    "            Resize(model_image_size),\n",
    "            CutoutPIL(cutout_factor=1/4), # cifar 32x32  随机把中间8x8正方形变成空白 \n",
    "            # CutoutPIL(cutout_factor=0.5),\n",
    "            RandAugment(),\n",
    "            \n",
    "            # resize\n",
    "            # center_crop\n",
    "            \n",
    "            # rescale\n",
    "            # normalize\n",
    "            \n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.hparams.test_transform = Compose(\n",
    "        [\n",
    "            # Resize(image_processor.size[\"height\"]),\n",
    "            # Resize(image_processor.crop_size[\"height\"]),\n",
    "            # # CenterCrop(image_processor.size[\"height\"]),\n",
    "            # CenterCrop(image_processor.crop_size[\"height\"]),\n",
    "            # Grayscale(num_output_channels=3),\n",
    "            \n",
    "            Resize(model_image_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycm/program_files/managers/conda/envs/hf_ai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_values([224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, BitImageProcessor\n",
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = BitImageProcessor.from_pretrained(model_checkpoint, use_fast=True)\n",
    "image_processor\n",
    "image_processor.crop_size.values() # height, width\n",
    "# image_processor.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.basics import patch\n",
    "@patch\n",
    "def get_lightning_data_module(self:ClassificationDataConfig):\n",
    "    if self.dataset_name == 'MNIST':\n",
    "        return MNISTDataModule.from_config(self)\n",
    "    elif self.dataset_name == 'CIFAR100':\n",
    "        return CIFAR100DataModule.from_config(self)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {self.dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"batch_size\":      1\n",
       "\"dataset_name\":    CIFAR100\n",
       "\"dataset_root\":    /home/ycm/repos/research/cv/cls/NamableClassify/data\n",
       "\"num_of_classes\":  100\n",
       "\"test_transform\":  None\n",
       "\"train_transform\": None\n",
       "\"train_val_split\": 0.9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_data = ClassificationDataConfig(dataset_name=\"CIFAR100\").get_lightning_data_module()\n",
    "lit_data.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"batch_size\":      1\n",
       "\"dataset_name\":    CIFAR100\n",
       "\"dataset_root\":    /home/ycm/repos/research/cv/cls/NamableClassify/data\n",
       "\"num_of_classes\":  100\n",
       "\"test_transform\":  Compose(\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       ")\n",
       "\"train_transform\": Compose(\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    <namable_classify.data.transforms.CutoutPIL object at 0x7d0ebf3047f0>\n",
       "    RandAugment(num_ops=2, magnitude=9, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       ")\n",
       "\"train_val_split\": 0.9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_data.set_transform_from_hf_image_preprocessor(image_processor)\n",
    "lit_data.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html\n",
    "# 这里的可视化不错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
