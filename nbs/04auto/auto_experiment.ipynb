{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto_experiment\n",
    "> automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/google-research/tuning_playbook for scientific research principles on meta parameters tuning. \n",
    "\n",
    "In addition to that guide, we also follow the paper \"Statistical Comparisons of Classifiers over Multiple Data Sets\", using statistical hypothesis testing to compare the performance of different models (produced by different meta parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp auto.experiment.infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Path' from 'namable_classify.utils' (/home/ye_canming/repos/novelties/cv/cls/NamableClassify/namable_classify/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnamable_classify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationTask, ClassificationTaskConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboguan_yuequ\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnucleus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoYueQuAlgorithm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/novelties/cv/cls/NamableClassify/namable_classify/core.py:68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationDataConfig, ClassificationDataModule\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mClassificationTaskConfig\u001b[39;00m(BaseModel):\n\u001b[1;32m     71\u001b[0m     experiment_project: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamableClassify Development\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# 为了发表什么论文，正在探索什么IDEA？\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/novelties/cv/cls/NamableClassify/namable_classify/data/__init__.py:11\u001b[0m\n\u001b[1;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple_pixel_flatten_transform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_names_of\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassificationDataConfig\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataloader_to_arrays\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassificationDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msksplit_for_torch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTorchVisionDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR100DataModule\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNISTDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR10DataModule\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFashionMNISTDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQMNISTDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKMNISTTDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# %% ../../nbs/01_data.ipynb 5\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_path, Path\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# %% ../../nbs/01_data.ipynb 6\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNIST, FashionMNIST, QMNIST, KMNIST\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Path' from 'namable_classify.utils' (/home/ye_canming/repos/novelties/cv/cls/NamableClassify/namable_classify/utils.py)"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from namable_classify.utils import runs_path\n",
    "from namable_classify.nucleus import ClassificationTask, ClassificationTaskConfig\n",
    "from boguan_yuequ.auto.nucleus import AutoYueQuAlgorithm\n",
    "import lightning as L\n",
    "from namable_classify.utils import runs_path\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder, BatchSizeFinder\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from namable_classify.utils import logger\n",
    "import torch\n",
    "# from clearml import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ye_canming/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'runs_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyTorchLightningPruningCallback\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLFlowLogger\n\u001b[0;32m----> 7\u001b[0m auto_exp_runs_path \u001b[38;5;241m=\u001b[39m \u001b[43mruns_path\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m auto_exp_runs_path\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_config\u001b[39m(\n\u001b[1;32m     12\u001b[0m     config: ClassificationTaskConfig,\n\u001b[1;32m     13\u001b[0m     trial: optuna\u001b[38;5;241m.\u001b[39mTrial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     tuning_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc1\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Seriously, 为了学术诚信规范，我们AI科研者不能用 \"test_acc1\" 来调参。\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     tuning_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'runs_path' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "auto_exp_runs_path = runs_path / \"auto_experiment\"\n",
    "auto_exp_runs_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "def run_with_config(\n",
    "    config: ClassificationTaskConfig,\n",
    "    trial: optuna.Trial = None,\n",
    "    tuning_metric=\"val_acc1\",  # Seriously, 为了学术诚信规范，我们AI科研者不能用 \"test_acc1\" 来调参。\n",
    "    tuning_mode=\"max\",\n",
    "):\n",
    "    logger.info(f\"running with config: {config}\")\n",
    "    L.seed_everything(config.experiment_index)\n",
    "    cls_task = ClassificationTask(config)\n",
    "    cls_task.print_model_pretty()\n",
    "    AutoYueQuAlgorithm(cls_task, config.yuequ, config.yuequ_pe)\n",
    "    # AutoYueQuAlgorithm(cls_task.cls_model, config.yuequ, config.yuequ_pe)\n",
    "    # Task.init(project_name=config.experiment_project, task_name=config.experiment_task)\n",
    "    # https://clear.ml/docs/latest/docs/guides/frameworks/pytorch_lightning/pytorch_lightning_example/\n",
    "\n",
    "    callbacks = [\n",
    "        # EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "        EarlyStopping(\n",
    "            monitor=tuning_metric,\n",
    "            mode=tuning_mode,\n",
    "            check_finite=True,\n",
    "            #   patience=5,\n",
    "            patience=10,\n",
    "            #   patience=6,\n",
    "            check_on_train_epoch_end=False,  # check on validation end\n",
    "            verbose=True,\n",
    "        ),\n",
    "        ModelSummary(max_depth=3),\n",
    "        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n",
    "        # StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "        # DeviceStatsMonitor(cpu_stats=True)\n",
    "        # LearningRateMonitor(),\n",
    "        # LearningRateFinder() # 有奇怪的bug\n",
    "        # BatchSizeFinder(init_val=32) # 用 \"power\" 减少调参不确定性; \n",
    "    ]\n",
    "    if trial is not None:\n",
    "        callbacks.append(PyTorchLightningPruningCallback(trial, monitor=tuning_metric))\n",
    "\n",
    "    lightning_loggers = [\n",
    "        TensorBoardLogger(save_dir=auto_exp_runs_path),\n",
    "        CSVLogger(save_dir=auto_exp_runs_path),\n",
    "        MLFlowLogger(experiment_name=f\"{config.experiment_project}/{config.experiment_task}\", \n",
    "                    #  .replace(\"_\", \"-\").replace(\" \", \"-\"), \n",
    "                     tracking_uri=\"http://10.103.10.55:5000\")\n",
    "        # WandbLogger(project=config.experiment_project, name=config.experiment_task),\n",
    "    ]\n",
    "    \n",
    "    torch.set_float32_matmul_precision(\"high\") # 尝试用 TF32\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=auto_exp_runs_path,\n",
    "        enable_checkpointing=True,\n",
    "        enable_model_summary=True,\n",
    "        num_sanity_val_steps=2,  # 防止 val 在训了好久train才发现崩溃\n",
    "        callbacks=callbacks\n",
    "        , max_epochs=30 \n",
    "        # , gradient_clip_val=1.0, gradient_clip_algorithm=\"value\"\n",
    "        ,\n",
    "        logger=lightning_loggers,\n",
    "        # , profiler=\"simple\"\n",
    "        # , fast_dev_run=True\n",
    "        # limit_train_batches=10, limit_val_batches=5\n",
    "        # strategy=\"ddp\", accelerator=\"gpu\", devices=4\n",
    "        accelerator=\"gpu\", devices=1, # 实验并行优于数据并行，尽量单卡训练\n",
    "        # accelerator=\"gpu\", devices=8, # 实验并行优于数据并行，尽量单卡训练\n",
    "        # precision=16 \n",
    "        # strategy=\"ddp\"\n",
    "    )\n",
    "    # batch size 和 learning rate\n",
    "    from lightning.pytorch.tuner import Tuner\n",
    "    tuner = Tuner(trainer)\n",
    "    found_batch_size = tuner.scale_batch_size(cls_task, datamodule=cls_task.lit_data, \n",
    "                                        #   mode='binsearch', \n",
    "                                          mode='power', \n",
    "                                          init_val=64)\n",
    "    cls_task.hparams.batch_size = found_batch_size = 64 * 8\n",
    "    linear_lr_scale = found_batch_size / 64\n",
    "    # linear_lr_scale *= 8 # 多卡训练\n",
    "    cls_task.hparams.learning_rate = config.learning_rate * linear_lr_scale\n",
    "    logger.info(f\"original learning rate: {config.learning_rate}, linear lr scale: {linear_lr_scale}, learning rate: {cls_task.hparams.learning_rate}\")\n",
    "\n",
    "    logger.info(f\"actual hyperparameters: {cls_task.hparams}\")\n",
    "    trainer.fit(cls_task, datamodule=cls_task.lit_data)\n",
    "    val_result = trainer.validate(cls_task, datamodule=cls_task.lit_data)\n",
    "    test_result = trainer.test(cls_task, datamodule=cls_task.lit_data)\n",
    "    # val_acc1 = val_result[0][\"val_acc1\"]\n",
    "    # test_acc1 = test_result[0][\"test_acc1\"]\n",
    "    # return val_acc1, test_acc1\n",
    "    return val_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from namable_classify.nucleus import ClassificationModelConfig, ClassificationTaskConfig, ClassificationDataConfig\n",
    "fixed_meta_parameters = ClassificationTaskConfig(\n",
    "    experiment_project = \"Homogeneous dwarf model is all you need for tuning pretrained giant model.\", \n",
    "    # experiment_name = \"Auto experiment\", \n",
    "    experiment_task = \"Auto experiment Stage 1 (single run, short epoches)\", \n",
    "    label_smoothing=0.1,  # 未必固定。\n",
    "    cls_model_config=ClassificationModelConfig(\n",
    "        # checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "    ), \n",
    "    dataset_config = ClassificationDataConfig(\n",
    "        # batch_size=64, # 经过前期经验, 这个方便站在61服务器跑, 大概10G显存。 固定基于这个调参\n",
    "        batch_size=16,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@dataclass\n",
    "class PostgresDatabaseConfig:\n",
    "    username: str\n",
    "    password: str\n",
    "    host: str\n",
    "    port: int\n",
    "    database_name: str\n",
    "    postgres_protocol: str = 'postgresql+psycopg2'\n",
    "\n",
    "    @property\n",
    "    def sqlalchemy_url(self) -> str:\n",
    "        return f'{self.postgres_protocol}://{self.username}:{self.password}@{self.host}:{self.port}/{self.database_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import json\n",
    "from namable_classify.utils import runs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "database_config_path = runs_path / 'database_config.json'\n",
    "with open(database_config_path, 'r') as f:\n",
    "    config = PostgresDatabaseConfig(**json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(config.sqlalchemy_url)\n",
    "# engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16589/618639724.py:24: ExperimentalWarning: WilcoxonPruner is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  pruner=WilcoxonPruner()\n",
      "[I 2024-11-01 05:01:55,532] A new study created in RDB with name: peft baselines benchmark\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fixed_meta_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m\n\u001b[1;32m     12\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m     13\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft baselines benchmark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# storage=sqlite_url, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# WilcoxonPruner(min_n_trials=10) # 不适合这个，这个 immediate 是fold的情况\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m study\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontributors\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYe Canming\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m study\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_meta_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mfixed_meta_parameters\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fixed_meta_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "from namable_classify.utils import runs_path\n",
    "from optuna.samplers import *\n",
    "from optuna.pruners import *\n",
    "# study_path = auto_exp_runs_path / \"optuna_studies.db\"\n",
    "# sqlite_url = f\"sqlite:///{study_path}\"\n",
    "# sqlite_url = f\"sqlite://{study_path}\"\n",
    "# TODO \n",
    "\n",
    "username = 'ycm'\n",
    "password = 'password'\n",
    "# host = 'localhost'\n",
    "host = '10.103.10.53'\n",
    "port = 5432\n",
    "database_name = 'namable_classify'\n",
    "postgres_url = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database_name}'\n",
    "# pip install psycopg2-binary \n",
    "# postgres_url = 'postgresql://myuser:mypassword@localhost/mydatabase'\n",
    "# TODO safety and privacy\n",
    "study = optuna.create_study(\n",
    "    # study_name=\"peft baselines benchmark\",  # old version\n",
    "    # study_name=\"peft baselines benchmark 11.3\", \n",
    "    study_name=\"peft baselines benchmark 11.7\", \n",
    "    # storage=sqlite_url, \n",
    "    storage=postgres_url, \n",
    "    load_if_exists=True, \n",
    "    direction=\"maximize\", \n",
    "    # https://pub.aimind.so/a-deep-dive-in-optunas-advance-features-2e495e71435c\n",
    "    # sampler=GPSampler(seed=42), \n",
    "    # sampler=TPESampler(seed=42), \n",
    "    # sampler=TPESampler(), \n",
    "    # https://github.com/optuna/optuna/issues/1647\n",
    "    sampler=CmaEsSampler(consider_pruned_trials = True), \n",
    "    pruner=HyperbandPruner()\n",
    "    # pruner=WilcoxonPruner()\n",
    "    # CmaEsSampler(seed=42),  我们实验数量应该小于1000\n",
    "    # WilcoxonPruner(min_n_trials=10) # 不适合这个，这个 immediate 是fold的情况\n",
    ")\n",
    "study.set_user_attr(\"contributors\", [\"Ye Canming\"])\n",
    "study.set_user_attr(\"fixed_meta_parameters\", fixed_meta_parameters.json())\n",
    "# 晚点再看\n",
    "# https://optuna-integration.readthedocs.io/en/stable/reference/generated/optuna_integration.MLflowCallback.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# study.optimize(objective, n_trials=100)\n",
    "study.optimize(lambda trial: objective(trial, num_of_repeated_experiments=1), \n",
    "               n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuequ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
