"""主训练脚本入口，调用各模块进行模型训练"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_main.ipynb.

# %% auto 0
__all__ = ['config', 'cls_task', 'trainer', 'tuner', 'found_batch_size', 'lr_finder', 'fig', 'new_lr']

# %% ../nbs/00_main.ipynb 5
from .core import ClassificationTask, ClassificationTaskConfig
config = ClassificationTaskConfig()
config
cls_task = ClassificationTask(config)
cls_task.print_model_pretty()
import torch
# cls_task.cls_model = torch.compile(cls_task.cls_model, mode='reduce-overhead')
#  fullgraph=True

# %% ../nbs/00_main.ipynb 6
import lightning as L
trainer = L.Trainer()
from lightning.pytorch.tuner import Tuner
tuner = Tuner(trainer)
found_batch_size = tuner.scale_batch_size(cls_task, datamodule=cls_task.lit_data, 
                                          mode='binsearch', 
                                          init_val=64)
# found_batch_size, cls_task.lit_data.hparams.batch_size
print(f"Found batch size: {found_batch_size}")

# %% ../nbs/00_main.ipynb 7
lr_finder = tuner.lr_find(cls_task, datamodule=cls_task.lit_data, max_lr=1e-2)
print(lr_finder.results)

fig = lr_finder.plot(suggest=True)
from matplotlib import pyplot as plt
from .utils import runs_figs_path
plt.savefig(runs_figs_path/'lr_finder.png')
# fig.show()
new_lr = lr_finder.suggestion()
# new_lr, cls_task.hparams.learning_rate
print("New learning rate: ", new_lr)

# %% ../nbs/00_main.ipynb 8
from .utils import runs_path
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger

trainer = L.Trainer(default_root_dir=runs_path, enable_checkpointing=True, 
                    enable_model_summary=True, 
                    num_sanity_val_steps=2, # 防止 val 在训了好久train才发现崩溃
                    callbacks=[
                        # EarlyStopping(monitor="val_loss", mode="min")
                        EarlyStopping(monitor="val_acc1", mode="max", check_finite=True, 
                                      patience=5, 
                                      check_on_train_epoch_end=False,  # check on validation end
                                      verbose=True),
                        ModelSummary(max_depth=3),
                        StochasticWeightAveraging(swa_lrs=1e-2), 
                        DeviceStatsMonitor(cpu_stats=True)
                               ]
                    
                    , gradient_clip_val=1.0, gradient_clip_algorithm="value"
                    , logger=[TensorBoardLogger(save_dir=runs_path/"tensorboard"), CSVLogger(save_dir=runs_path)]
                    # , profiler="simple"
                    # , fast_dev_run=True
                    # limit_train_batches=10, limit_val_batches=5
                    # strategy="ddp", accelerator="gpu", devices=4
                    )
trainer.fit(cls_task, datamodule=cls_task.lit_data)
