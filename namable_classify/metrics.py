# AUTOGENERATED! DO NOT EDIT! File to edit: ../src/notebooks/03_metrics.ipynb.

# %% auto 0
__all__ = ['except_roc_auc_score', 'compute_classification_metrics', 'per_class_accuracy', 'get_top_bottom_k_classes',
           'list_i_terms', 'draw_classification_metrics']

# %% ../src/notebooks/03_metrics.ipynb 5
from .infra import default_on_exception, ensure_array
import torch.nn.functional as F
import numpy as np
import torch
from sklearn.metrics import roc_auc_score, top_k_accuracy_score, matthews_corrcoef, f1_score, precision_score, recall_score, log_loss, balanced_accuracy_score, cohen_kappa_score, hinge_loss, accuracy_score

from .infra import MuteWarnings
import warnings
except_roc_auc_score = default_on_exception(default_value=-1)(roc_auc_score)

def compute_classification_metrics(
    y_true: np.ndarray,  # 1d array-like, or label indicator array / sparse matrix
    y_pred_logits: np.ndarray = None,  # label indicator array / sparse matrix
    logits_to_prob: bool = False,  # function to convert logits to probabilities
    y_pred: np.ndarray = None,  # predicted labels, if None, will be computed from logits
    labels:list[int|str]|None = None,  # list of labels
    supress_warnings: bool = True,  # whether to suppress warnings
    y_pred_metrics_only: bool = False,  # whether to compute only y_pred related metrics
):
        
    if supress_warnings:
        mute = MuteWarnings()
        mute.mute()
    if y_pred_logits is None:
        assert y_pred_metrics_only == True, "y_pred_logits is None, we can only compute y_pred related metrics! "
        assert y_pred is not None, "y_pred_logits is None, y_pred should be specified! "
        # warnings.warn("y_pred_logits is None, will compute y_pred related metrics only! ")
    y_true = ensure_array(y_true)
    if not y_pred_metrics_only:
        y_pred_logits = ensure_array(y_pred_logits)
        # print(type(y_pred_logits)) # <class 'numpy.ndarray'>
        # y_pred_probs = softmax(y_pred_logits)# label indicator array / sparse matrix
        y_pred_probs = (
            np.array(F.softmax(torch.Tensor(y_pred_logits), dim=1))
            if logits_to_prob
            else y_pred_logits
        )  # label indicator array / sparse matrix
    other_res = {}
    if y_pred is None:
        # 必然有 y_pred_logits
        assert y_pred_logits is not None, "y_pred_logits is None, cannot derive y_pred! "
        y_pred = np.argmax(y_pred_logits, axis=1)
    else:
        # 额外计算一个acc
        if not y_pred_metrics_only:
            warnings.warn("y_pred is specified since it may be different from argmax(y_pred_logits), this may happen to prob SVM. ")
        other_res["acc1_pred"] = accuracy_score(y_true, y_pred)
        
        
    # target_names = labels # dataset['train'].features[label_column_name].names
    # report_dict = classification_report(y_true, y_pred_probs, target_names=target_names, output_dict=True)
    
    if not y_pred_metrics_only:
        top_k_res = {
            f"acc{k}": top_k_accuracy_score(y_true, y_pred_probs, k=k, labels=labels)
            for k in [1, 2, 3, 5, 10, 20]
        }
        prob_res = dict(
            # roc_auc=roc_auc_score(
            roc_auc=except_roc_auc_score(
                y_true, y_pred_probs, average="macro", multi_class="ovr", labels=labels
            ),  # ovr更难一些，会不平衡
            hinge_loss=hinge_loss(y_true, y_pred_probs, labels=labels),
            log_loss=log_loss(
                y_true,
                y_pred_probs,
                labels=labels
                ),
            )
    else: 
        top_k_res = {}
        prob_res = {}

    pred_res = dict(
        matthews_corrcoef=matthews_corrcoef(y_true, y_pred),
        f1=f1_score(y_true, y_pred, average="macro", labels=labels),
        precision=precision_score(y_true, y_pred, average="macro", labels=labels),
        recall=recall_score(y_true, y_pred, average="macro", labels=labels),
        balanced_accuracy=balanced_accuracy_score(y_true, y_pred),
        cohen_kappa=cohen_kappa_score(y_true, y_pred, labels=labels),
    )
    if supress_warnings:
        mute.resume()
    
    # return top_k_res| balance_res| report_dict
    return top_k_res | pred_res | prob_res | other_res

# %% ../src/notebooks/03_metrics.ipynb 7
import numpy as np
from sklearn.metrics import accuracy_score

def per_class_accuracy(y_true, y_pred, labels=None):
    # 如果未提供 labels，则从 y_true 中获取
    if labels is None:
        labels = np.unique(y_true)
    
    # 计算每个类别的准确率
    per_class_acc = {}
    for cls in labels:
        idx = y_true == cls
        acc = accuracy_score(y_true[idx], y_pred[idx])
        per_class_acc[cls] = acc
    
    return per_class_acc
def get_top_bottom_k_classes(per_class_acc, k):
    # 将字典按准确率排序（从高到低）
    sorted_acc = sorted(per_class_acc.items(), key=lambda item: item[1], reverse=True)
    
    # 获取准确率最高的前 k 个类别
    top_k = sorted_acc[:k]
    
    # 获取准确率最低的前 k 个类别
    bottom_k = sorted_acc[-k:]
    
    return top_k, bottom_k

def list_i_terms(lst, i=0):
    return [item[i] for item in lst]

# %% ../src/notebooks/03_metrics.ipynb 9
import scikitplot as skplt
import matplotlib.pyplot as plt

def draw_classification_metrics(
    y_true: np.ndarray,  # 1d array-like, or label indicator array / sparse matrix
    y_pred_logits: np.ndarray = None,  # label indicator array / sparse matrix
    logits_to_prob: bool = False,  # function to convert logits to probabilities
    y_pred: np.ndarray = None,  # predicted labels, if None, will be computed from logits
    labels:list[int|str]|None = None,  # list of labels
    supress_warnings: bool = True,  # whether to suppress warnings
    y_pred_metrics_only: bool = False,  # whether to compute only y_pred related metrics
    max_classes_to_plot: int = 6,  # maximum number of classes to plot, to avoid overplotting
):
    # 鲁棒性处理
    if supress_warnings:
        mute = MuteWarnings()
        mute.mute()
    if y_pred_logits is None:
        assert y_pred_metrics_only == True, "y_pred_logits is None, we can only compute y_pred related metrics! "
        assert y_pred is not None, "y_pred_logits is None, y_pred should be specified! "
        # warnings.warn("y_pred_logits is None, will compute y_pred related metrics only! ")
    y_true = ensure_array(y_true)
    if not y_pred_metrics_only:
        y_pred_logits = ensure_array(y_pred_logits)
        # print(type(y_pred_logits)) # <class 'numpy.ndarray'>
        # y_pred_probs = softmax(y_pred_logits)# label indicator array / sparse matrix
        y_pred_probs = (
            np.array(F.softmax(torch.Tensor(y_pred_logits), dim=1))
            if logits_to_prob
            else y_pred_logits
        )  # label indicator array / sparse matrix
    
    # TODO 图片收集不是很优雅
    figures = []
    figure_names = []
    if y_pred is None:
        # 必然有 y_pred_logits
        assert y_pred_logits is not None, "y_pred_logits is None, cannot derive y_pred! "
        y_pred = np.argmax(y_pred_logits, axis=1)
    else:
        # 额外计算一个acc
        if not y_pred_metrics_only:
            warnings.warn("y_pred is specified since it may be different from argmax(y_pred_logits), this may happen to prob SVM. ")
            
    
    # 核心功能

    # print(y_true.shape, y_pred_probs.shape, y_pred.shape)
    if not y_pred_metrics_only:
        # if len(labels) <= max_classes_to_plot:
        #     classes_to_plot = labels
        # else:
        #     k = max_classes_to_plot // 2
        #     per_class_acc = per_class_accuracy(y_true, y_pred, labels)
        #     top_k, bottom_k = get_top_bottom_k_classes(per_class_acc, k)
        #     classes_to_plot = list_i_terms(top_k) + list_i_terms(bottom_k)
            
        
        # fig = plt.figure()
        # skplt.metrics.plot_roc(y_true, y_pred_probs, plot_micro=True, plot_macro=True, 
        #                        classes_to_plot=classes_to_plot)
        # figures.append(fig)
        # figure_names.append("roc_curve")
        
        # fig = plt.figure()
        # skplt.metrics.plot_precision_recall(y_true, y_pred_probs, plot_micro=True, 
        #                                     classes_to_plot=classes_to_plot)
        # figures.append(fig)
        # figure_names.append("precision_recall_curve")
        

    
        if len(labels) == 2:
            fig = plt.figure()
            skplt.metrics.plot_ks_statistic(y_true, y_pred_probs)
            figures.append(fig)
            figure_names.append("ks_statistic")
            fig = plt.figure()
            skplt.metrics.plot_cumulative_gain(y_true, y_pred_probs)
            figures.append(fig)
            figure_names.append("cumulative_gain")
            fig = plt.figure()
            skplt.metrics.plot_lift_curve(y_true, y_pred_probs)
            figures.append(fig)
            figure_names.append("lift_curve")
    fig = plt.figure()
    skplt.metrics.plot_confusion_matrix(y_true, y_pred)
    figures.append(fig)
    figure_names.append("confusion_matrix")
    
    if supress_warnings:
        mute.resume()

    return figures, figure_names
