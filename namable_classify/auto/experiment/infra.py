"""automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../src/notebooks/04auto/auto_experiment.ipynb.

# %% auto 0
__all__ = ['auto_exp_runs_path', 'checkpoint_path', 'fixed_meta_parameters', 'study_path', 'sqlite_url', 'database_config_path',
           'run_with_config', 'PostgresDatabaseConfig']

# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 4
import os
os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"

# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 5
from ...infra import runs_path
from ...nucleus import ClassificationTask, ClassificationTaskConfig
from boguan_yuequ.auto.nucleus import AutoYueQuAlgorithm
import lightning as L
from ...infra import runs_path
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder, BatchSizeFinder
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger
import optuna
from optuna.integration import PyTorchLightningPruningCallback
from lightning.pytorch.loggers import MLFlowLogger
from ...infra import logger
import torch
# from clearml import Task
from lightning.pytorch.profilers import AdvancedProfiler



# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 8
import json
from pathlib import Path
from matplotlib import pyplot as plt
from lightning.pytorch.callbacks import ModelCheckpoint

# 重要
# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399


auto_exp_runs_path = runs_path / "auto_experiment"
auto_exp_runs_path.mkdir(exist_ok=True, parents=True)

checkpoint_path = "/ccfa/ycm/namable_classify"



def run_with_config(
    config: ClassificationTaskConfig,
    trial: optuna.Trial = None,
    tuning_metric="val_acc1",  # Seriously, 为了学术诚信规范，我们AI科研者不能用 "test_acc1" 来调参。
    tuning_mode="max", 
    batch_size=-1,
    need_reproduce=False, # 调参阶段不需要复现，需要快速迭代
    running_path:Path = auto_exp_runs_path,
    checkpoint_path:str = checkpoint_path,
    search_learning_rate = False,
    # fast_dev_run=False, # 行为不太一样
    profile = False,
    limit_train_batches = None,
    limit_val_batches = None,
    limit_test_batches = None,
    max_epochs = 30,
    **kwargs
):
    logger.info(f"running with config: {config}")
    if need_reproduce:
        L.seed_everything(config.experiment_index)
    cls_task = ClassificationTask(config)
    # cls_task.print_model_pretty()
    # AutoYueQuAlgorithm(cls_task, config.yuequ, config.yuequ_pe)
    cls_task.cls_model.backbone = AutoYueQuAlgorithm(cls_task.cls_model.backbone,
        config.yuequ, config.yuequ_pe).adapted_module
    cls_task.print_trainable_parameters()
    
    # AutoYueQuAlgorithm(cls_task.cls_model, config.yuequ, config.yuequ_pe)
    # Task.init(project_name=config.experiment_project, task_name=config.experiment_task)
    # https://clear.ml/docs/latest/docs/guides/frameworks/pytorch_lightning/pytorch_lightning_example/
    best_checkpoint_callback = ModelCheckpoint(
            save_top_k=1, # 只保存一个最好的模型
            monitor=tuning_metric,
            mode=tuning_mode,
            dirpath=checkpoint_path,
            filename=f'{config.yuequ}-{config.dataset_config.dataset_name}'+'-{epoch:02d}-{' + tuning_metric + ':.2f}',
        )
    callbacks = [
        best_checkpoint_callback, 
        # ModelCheckpoint(
        #     save_top_k=1, # 只保存最后一个
        #     # monitor="global_step",
        #     monitor="epoch",
        #     mode="max",
        #     dirpath=checkpoint_path,
        #     save_last = 'link', # 软链接过去，其实只保存一个就是那个
        #     filename=f'{config.yuequ}-{config.dataset_config.dataset_name}'+'-{epoch:02d}',
        # ),  
        EarlyStopping(
            monitor=tuning_metric,
            mode=tuning_mode,
            check_finite=True,
            #   patience=5,
            patience=10,
            #   patience=6,
            check_on_train_epoch_end=False,  # check on validation end
            verbose=True,
        ),
        ModelSummary(max_depth=3),
        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
        # StochasticWeightAveraging(swa_lrs=1e-2),
        # DeviceStatsMonitor(cpu_stats=True)
        # LearningRateMonitor(),
        # LearningRateFinder() # 有奇怪的bug
        # BatchSizeFinder(init_val=32) # 用 "power" 减少调参不确定性; 
    ]
    if trial is not None:
        callbacks.append(PyTorchLightningPruningCallback(trial, monitor=tuning_metric))

    lightning_loggers = [
        TensorBoardLogger(save_dir=running_path, log_graph=True),
        CSVLogger(save_dir=running_path),
        # MLFlowLogger(experiment_name=f"{config.experiment_project}/{config.experiment_task}", 
        #             #  .replace("_", "-").replace(" ", "-"), 
        #              tracking_uri="http://10.103.10.55:5000")
        # WandbLogger(project=config.experiment_project, name=config.experiment_task),
    ]
    
    torch.set_float32_matmul_precision("high") # 这个只是对矩阵乘法生效，只是为了加速
    trainer = L.Trainer(
        default_root_dir=running_path,
        # enable_checkpointing=True,
        enable_model_summary=True,
        num_sanity_val_steps=2,  # 防止 val 在训了好久train才发现崩溃
        callbacks=callbacks
        , max_epochs=max_epochs 
        # , gradient_clip_val=1.0, gradient_clip_algorithm="value"
        ,
        logger=lightning_loggers,
        # , profiler="simple"
        # , fast_dev_run=True
        # limit_train_batches=10, limit_val_batches=5
        # strategy="ddp", accelerator="gpu", devices=4
        accelerator="gpu", devices=1, # 实验并行优于数据并行，尽量单卡训练
        # accelerator="gpu", devices=8, 
        # precision=16 
        # accelerator="gpu", devices=2, strategy="deepspeed_stage_2_offload", precision="16-mixed",
        # strategy="ddp"
        benchmark=not need_reproduce,
        deterministic=need_reproduce,
        # profiler = AdvancedProfiler(dirpath=".", 
        #                             filename=(running_path/"profiler_logs.txt").as_posix(), # 必须str
        #                             ) # speed check
        profiler='simple' if profile else None,
        # fast_dev_run = fast_dev_run, 
        limit_train_batches=limit_train_batches,
        limit_val_batches=limit_val_batches,
        limit_test_batches=limit_test_batches,
        **kwargs
    )
    trainer_log_dir_path = Path(trainer.log_dir)
    trainer_log_dir_path.mkdir(exist_ok=True, parents=True)
    # if profile:
    #     profiler = AdvancedProfiler(dirpath=trainer.log_dir, 
    #                                 filename="perf_logs"), 
    #     trainer.profiler = profiler
    
    # batch size 和 learning rate
    if batch_size == -1 or search_learning_rate:
        from lightning.pytorch.tuner import Tuner
        tuner_trainer = L.Trainer()
        tuner = Tuner(tuner_trainer)
    if batch_size == -1:
        # Batch size finder is not yet supported for DDP or any of its variations, it is coming soon.
        
        found_batch_size = tuner.scale_batch_size(cls_task, datamodule=cls_task.lit_data, 
                                            #   mode='binsearch', 
                                            mode='power', 
                                            init_val=config.dataset_config.batch_size)
    else:
        found_batch_size = batch_size
    cls_task.hparams.batch_size = found_batch_size
    
    # config.dataset_config.batch_size = found_batch_size
    # cls_task = ClassificationTask(config) # 防止状态错误。
    # cls_task.hparams.batch_size = found_batch_size
    
    # https://github.com/davidtvs/pytorch-lr-finder 
    # 更加精细控制
    if search_learning_rate:
        
        lr_finder = tuner.lr_find(cls_task, datamodule=cls_task.lit_data, 
                                  max_lr=1e-2, min_lr=1e-7, 
                                  num_training=100)
        print(lr_finder.results)
        fig = lr_finder.plot(suggest=True)
        # fig.show()
        plt.savefig(trainer_log_dir_path/"lr_finder.png")
        lightning_loggers[0].experiment.add_figure("lr_finder", fig, global_step=0) # tensorboard
        new_lr = lr_finder.suggestion()
        
        cls_task.hparams.learning_rate = new_lr
        
        # config.learning_rate = new_lr
        # cls_task = ClassificationTask(config) # 防止状态错误
    else:
        linear_lr_scale = found_batch_size / 64
        cls_task.hparams.learning_rate = config.learning_rate * linear_lr_scale
        logger.info(f"original learning rate: {config.learning_rate}, linear lr scale: {linear_lr_scale}, learning rate: {cls_task.hparams.learning_rate}")

    

    logger.info(f"actual hyperparameters: {cls_task.hparams}")
    
    trainer.fit(cls_task, datamodule=cls_task.lit_data)
    
    # 重新计算 Early Stop的时候，最好的那一个模型
    # best_cls_task_model = cls_task.load_from_checkpoint(best_checkpoint_callback.best_model_path)
    # val_result = trainer.validate(best_cls_task_model, datamodule=cls_task.lit_data)
    # test_result = trainer.test(best_cls_task_model, datamodule=cls_task.lit_data)
    
    val_result = trainer.validate(ckpt_path='best', datamodule=cls_task.lit_data)
    test_result = trainer.test(ckpt_path='best', datamodule=cls_task.lit_data)
    
    results = val_result[0] | test_result[0]
    
    results_json_path = trainer_log_dir_path/"results.json"
    with open(results_json_path, "w") as f:
        json.dump(results, f, indent=4)
    
        
    # val_acc1 = val_result[0]["val_acc1"]
    # test_acc1 = test_result[0]["test_acc1"]
    # return val_acc1, test_acc1
    return val_result, test_result

# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 9
from ...nucleus import ClassificationModelConfig, ClassificationTaskConfig, ClassificationDataConfig
fixed_meta_parameters = ClassificationTaskConfig(
    experiment_project = "Homogeneous dwarf model is all you need for tuning pretrained giant model.", 
    # experiment_name = "Auto experiment", 
    experiment_task = "Auto experiment Stage 1 (single run, short epoches)", 
    label_smoothing=0.1,  # 未必固定。
    cls_model_config=ClassificationModelConfig(
        # checkpoint = "google/vit-base-patch16-224-in21k"
    ), 
    dataset_config = ClassificationDataConfig(
        # batch_size=64, # 经过前期经验, 这个方便站在61服务器跑, 大概10G显存。 固定基于这个调参
        batch_size=16,
    )
)


# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 10
from dataclasses import dataclass


# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 11
study_path = auto_exp_runs_path / "optuna_studies.db"
sqlite_url = f"sqlite:///{study_path}"
# sqlite_url = f"sqlite://{study_path}"

# pip install psycopg2-binary 

@dataclass
class PostgresDatabaseConfig:
    username: str
    password: str
    host: str
    port: int
    database_name: str
    postgres_protocol: str = 'postgresql+psycopg2'

    @property
    def sqlalchemy_url(self) -> str:
#    postgres_url = 'postgresql://myuser:mypassword@localhost/mydatabase'
    
        return f'{self.postgres_protocol}://{self.username}:{self.password}@{self.host}:{self.port}/{self.database_name}'


# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 12
import json
from ...infra import runs_path

# %% ../../../src/notebooks/04auto/auto_experiment.ipynb 13
database_config_path = runs_path / 'database_config.json'
with open(database_config_path, 'r') as f:
    config = PostgresDatabaseConfig(**json.load(f))
