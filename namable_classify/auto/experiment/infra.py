"""automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/04auto/auto_experiment.ipynb.

# %% auto 0
__all__ = ['auto_exp_runs_path', 'fixed_meta_parameters', 'study_path', 'sqlite_url', 'database_config_path', 'study',
           'run_with_config', 'PostgresDatabaseConfig']

# %% ../../../nbs/04auto/auto_experiment.ipynb 4
import os
os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"

# %% ../../../nbs/04auto/auto_experiment.ipynb 5
from ...infra import runs_path
from ...nucleus import ClassificationTask, ClassificationTaskConfig
from boguan_yuequ.auto.nucleus import AutoYueQuAlgorithm
import lightning as L
from ...infra import runs_path
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder, BatchSizeFinder
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger
import optuna
from optuna.integration import PyTorchLightningPruningCallback
from lightning.pytorch.loggers import MLFlowLogger
from ...infra import logger
import torch
# from clearml import Task

# %% ../../../nbs/04auto/auto_experiment.ipynb 6
auto_exp_runs_path = runs_path / "auto_experiment"
auto_exp_runs_path.mkdir(exist_ok=True, parents=True)


def run_with_config(
    config: ClassificationTaskConfig,
    trial: optuna.Trial = None,
    tuning_metric="val_acc1",  # Seriously, 为了学术诚信规范，我们AI科研者不能用 "test_acc1" 来调参。
    tuning_mode="max",
):
    logger.info(f"running with config: {config}")
    L.seed_everything(config.experiment_index)
    cls_task = ClassificationTask(config)
    cls_task.print_model_pretty()
    AutoYueQuAlgorithm(cls_task, config.yuequ, config.yuequ_pe)
    # AutoYueQuAlgorithm(cls_task.cls_model, config.yuequ, config.yuequ_pe)
    # Task.init(project_name=config.experiment_project, task_name=config.experiment_task)
    # https://clear.ml/docs/latest/docs/guides/frameworks/pytorch_lightning/pytorch_lightning_example/

    callbacks = [
        # EarlyStopping(monitor="val_loss", mode="min")
        EarlyStopping(
            monitor=tuning_metric,
            mode=tuning_mode,
            check_finite=True,
            #   patience=5,
            patience=10,
            #   patience=6,
            check_on_train_epoch_end=False,  # check on validation end
            verbose=True,
        ),
        ModelSummary(max_depth=3),
        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
        # StochasticWeightAveraging(swa_lrs=1e-2),
        # DeviceStatsMonitor(cpu_stats=True)
        # LearningRateMonitor(),
        # LearningRateFinder() # 有奇怪的bug
        # BatchSizeFinder(init_val=32) # 用 "power" 减少调参不确定性; 
    ]
    if trial is not None:
        callbacks.append(PyTorchLightningPruningCallback(trial, monitor=tuning_metric))

    lightning_loggers = [
        TensorBoardLogger(save_dir=auto_exp_runs_path, log_graph=True),
        CSVLogger(save_dir=auto_exp_runs_path),
        MLFlowLogger(experiment_name=f"{config.experiment_project}/{config.experiment_task}", 
                    #  .replace("_", "-").replace(" ", "-"), 
                     tracking_uri="http://10.103.10.55:5000")
        # WandbLogger(project=config.experiment_project, name=config.experiment_task),
    ]
    
    torch.set_float32_matmul_precision("high") # 尝试用 TF32
    trainer = L.Trainer(
        default_root_dir=auto_exp_runs_path,
        enable_checkpointing=True,
        enable_model_summary=True,
        num_sanity_val_steps=2,  # 防止 val 在训了好久train才发现崩溃
        callbacks=callbacks
        , max_epochs=30 
        # , gradient_clip_val=1.0, gradient_clip_algorithm="value"
        ,
        logger=lightning_loggers,
        # , profiler="simple"
        # , fast_dev_run=True
        # limit_train_batches=10, limit_val_batches=5
        # strategy="ddp", accelerator="gpu", devices=4
        accelerator="gpu", devices=1, # 实验并行优于数据并行，尽量单卡训练
        # accelerator="gpu", devices=8, # 实验并行优于数据并行，尽量单卡训练
        # precision=16 
        # strategy="ddp"
    )
    # batch size 和 learning rate
    from lightning.pytorch.tuner import Tuner
    tuner = Tuner(trainer)
    found_batch_size = tuner.scale_batch_size(cls_task, datamodule=cls_task.lit_data, 
                                        #   mode='binsearch', 
                                          mode='power', 
                                          init_val=64)
    cls_task.hparams.batch_size = found_batch_size = 64 * 8
    linear_lr_scale = found_batch_size / 64
    # linear_lr_scale *= 8 # 多卡训练
    cls_task.hparams.learning_rate = config.learning_rate * linear_lr_scale
    logger.info(f"original learning rate: {config.learning_rate}, linear lr scale: {linear_lr_scale}, learning rate: {cls_task.hparams.learning_rate}")

    logger.info(f"actual hyperparameters: {cls_task.hparams}")
    trainer.fit(cls_task, datamodule=cls_task.lit_data)
    val_result = trainer.validate(cls_task, datamodule=cls_task.lit_data)
    test_result = trainer.test(cls_task, datamodule=cls_task.lit_data)
    # val_acc1 = val_result[0]["val_acc1"]
    # test_acc1 = test_result[0]["test_acc1"]
    # return val_acc1, test_acc1
    return val_result, test_result

# %% ../../../nbs/04auto/auto_experiment.ipynb 7
from ...nucleus import ClassificationModelConfig, ClassificationTaskConfig, ClassificationDataConfig
fixed_meta_parameters = ClassificationTaskConfig(
    experiment_project = "Homogeneous dwarf model is all you need for tuning pretrained giant model.", 
    # experiment_name = "Auto experiment", 
    experiment_task = "Auto experiment Stage 1 (single run, short epoches)", 
    label_smoothing=0.1,  # 未必固定。
    cls_model_config=ClassificationModelConfig(
        # checkpoint = "google/vit-base-patch16-224-in21k"
    ), 
    dataset_config = ClassificationDataConfig(
        # batch_size=64, # 经过前期经验, 这个方便站在61服务器跑, 大概10G显存。 固定基于这个调参
        batch_size=16,
    )
)


# %% ../../../nbs/04auto/auto_experiment.ipynb 8
from dataclasses import dataclass


# %% ../../../nbs/04auto/auto_experiment.ipynb 9
study_path = auto_exp_runs_path / "optuna_studies.db"
sqlite_url = f"sqlite:///{study_path}"
# sqlite_url = f"sqlite://{study_path}"

# pip install psycopg2-binary 

@dataclass
class PostgresDatabaseConfig:
    username: str
    password: str
    host: str
    port: int
    database_name: str
    postgres_protocol: str = 'postgresql+psycopg2'

    @property
    def sqlalchemy_url(self) -> str:
#    postgres_url = 'postgresql://myuser:mypassword@localhost/mydatabase'
    
        return f'{self.postgres_protocol}://{self.username}:{self.password}@{self.host}:{self.port}/{self.database_name}'


# %% ../../../nbs/04auto/auto_experiment.ipynb 10
import json
from ...infra import runs_path

# %% ../../../nbs/04auto/auto_experiment.ipynb 11
database_config_path = runs_path / 'database_config.json'
with open(database_config_path, 'r') as f:
    config = PostgresDatabaseConfig(**json.load(f))

# %% ../../../nbs/04auto/auto_experiment.ipynb 13
from optuna.samplers import *
from optuna.pruners import *


study = optuna.create_study(
    # study_name="peft baselines benchmark",  # old version
    # study_name="peft baselines benchmark 11.3", 
    # study_name="peft baselines benchmark 11.7", 
    study_name="peft baselines benchmark 11.29", 
    # storage=config.sqlalchemy_url, 
    storage=sqlite_url, 
    load_if_exists=True, 
    direction="maximize", 
    # https://pub.aimind.so/a-deep-dive-in-optunas-advance-features-2e495e71435c
    # sampler=GPSampler(seed=42), 
    # sampler=TPESampler(seed=42), 
    # sampler=TPESampler(), 
    # https://github.com/optuna/optuna/issues/1647
    sampler=CmaEsSampler(consider_pruned_trials = True), 
    pruner=HyperbandPruner()
    # pruner=WilcoxonPruner()
    # CmaEsSampler(seed=42),  我们实验数量应该小于1000
    # WilcoxonPruner(min_n_trials=10) # 不适合这个，这个 immediate 是fold的情况
)
study.set_user_attr("contributors", ["Ye Canming"])
study.set_user_attr("fixed_meta_parameters", fixed_meta_parameters.json())
# 晚点再看
# https://optuna-integration.readthedocs.io/en/stable/reference/generated/optuna_integration.MLflowCallback.html
