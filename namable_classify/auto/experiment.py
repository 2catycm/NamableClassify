"""automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_auto_experiment.ipynb.

# %% auto 0
__all__ = ['auto_exp_runs_path', 'fixed_meta_parameters', 'study_results', 'backbone_name2pe', 'peft_to_try', 'delta_to_try',
           'yuequ_to_try', 'postgres_url', 'study', 'run_with_config', 'objective']

# %% ../../nbs/02_auto_experiment.ipynb 4
import os
os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"

# %% ../../nbs/02_auto_experiment.ipynb 5
from ..core import ClassificationTask, ClassificationTaskConfig
from boguan_yuequ.auto.nucleus import AutoYueQuAlgorithm
import lightning as L
from ..utils import runs_path
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder, BatchSizeFinder
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger

# %% ../../nbs/02_auto_experiment.ipynb 8
# from clearml import Task
import optuna
from optuna.integration import PyTorchLightningPruningCallback

auto_exp_runs_path = runs_path / "auto_experiment"
auto_exp_runs_path.mkdir(exist_ok=True, parents=True)


def run_with_config(
    config: ClassificationTaskConfig,
    trial: optuna.Trial = None,
    tuning_metric="val_acc1",  # Seriously, 为了学术诚信规范，我们AI科研者不能用 "test_acc1" 来调参。
    tuning_mode="max",
):
    L.seed_everything(config.experiment_index)
    cls_task = ClassificationTask(config)
    cls_task.print_model_pretty()
    AutoYueQuAlgorithm(cls_task, config.yuequ, config.yuequ_pe)
    # AutoYueQuAlgorithm(cls_task.cls_model, config.yuequ, config.yuequ_pe)
    # Task.init(project_name=config.experiment_project, task_name=config.experiment_task)
    # https://clear.ml/docs/latest/docs/guides/frameworks/pytorch_lightning/pytorch_lightning_example/

    callbacks = [
        # EarlyStopping(monitor="val_loss", mode="min")
        EarlyStopping(
            monitor=tuning_metric,
            mode=tuning_mode,
            check_finite=True,
            #   patience=5,
            patience=10,
            #   patience=6,
            check_on_train_epoch_end=False,  # check on validation end
            verbose=True,
        ),
        ModelSummary(max_depth=3),
        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
        # StochasticWeightAveraging(swa_lrs=1e-2),
        # DeviceStatsMonitor(cpu_stats=True)
        # LearningRateMonitor(),
        # LearningRateFinder() # 有奇怪的bug
        BatchSizeFinder(init_val=32) # 用 "power" 减少调参不确定性; 
    ]
    # if trial is not None:
    #     callbacks.append(PyTorchLightningPruningCallback(trial, monitor=tuning_metric))

    logger = [
        TensorBoardLogger(save_dir=auto_exp_runs_path),
        CSVLogger(save_dir=auto_exp_runs_path),
        WandbLogger(project=config.experiment_project, name=config.experiment_task),
    ]

    trainer = L.Trainer(
        default_root_dir=auto_exp_runs_path,
        enable_checkpointing=True,
        enable_model_summary=True,
        num_sanity_val_steps=2,  # 防止 val 在训了好久train才发现崩溃
        callbacks=callbacks
        # , max_epochs=15
        # , gradient_clip_val=1.0, gradient_clip_algorithm="value"
        ,
        logger=logger,
        # , profiler="simple"
        # , fast_dev_run=True
        # limit_train_batches=10, limit_val_batches=5
        # strategy="ddp", accelerator="gpu", devices=4
    )

    trainer.fit(cls_task, datamodule=cls_task.lit_data)
    val_result = trainer.validate(cls_task, datamodule=cls_task.lit_data)
    test_result = trainer.test(cls_task, datamodule=cls_task.lit_data)
    # val_acc1 = val_result[0]["val_acc1"]
    # test_acc1 = test_result[0]["test_acc1"]
    # return val_acc1, test_acc1
    return val_result, test_result

# %% ../../nbs/02_auto_experiment.ipynb 9
from ..core import ClassificationModelConfig, ClassificationTaskConfig, ClassificationDataConfig
fixed_meta_parameters = ClassificationTaskConfig(
    experiment_project = "Homogeneous dwarf model is all you need for tuning pretrained giant model.", 
    experiment_name = "Auto experiment", 
    label_smoothing=0.1,  # 未必固定。
    cls_model_config=ClassificationModelConfig(
        # checkpoint = "google/vit-base-patch16-224-in21k"
    ), 
    dataset_config = ClassificationDataConfig(
        # batch_size=64, # 经过前期经验, 这个方便站在61服务器跑, 大概10G显存。 固定基于这个调参
        batch_size=16,
    )
)


# %% ../../nbs/02_auto_experiment.ipynb 11
study_results = [] # 准备装入 dict

# %% ../../nbs/02_auto_experiment.ipynb 12
# 需要跑哪些backbone 和 对应的 pe呢？
from boguan_yuequ.benchmarking import pe_list_tiny_for_all_size, backbone_names
# from transformers import AutoModel
# # tiny+tiny vs tiny full vs tiny full_lora 也是有意义的对比，所以不做截断。
# for config, pe in zip(configs, pe_list_tiny_for_all_size):
#     model = AutoModel.from_pretrained(config)
#     yuequ = AutoYueQuAlgorithm(model, 'lora', pe)
#     model = yuequ.adapted_model
#     pe = yuequ.pe
backbone_name2pe = {backbone_name:pe for pe, backbone_name in zip(pe_list_tiny_for_all_size, backbone_names)}

# %% ../../nbs/02_auto_experiment.ipynb 14
# 需要跑哪些算法呢？
# from boguan_yuequ.auto import huggingface_peft_budget_config_key, thu_nlp_opendelta_budget_config_key
from boguan_yuequ.auto.integrations.peft import huggingface_peft_budget_config_key
from boguan_yuequ.auto.integrations.opendelta import thunlp_opendelta_budget_config_key

peft_to_try = [k.name for k in huggingface_peft_budget_config_key.keys()]
delta_to_try = [k for k in thunlp_opendelta_budget_config_key.keys() if k.upper() not in peft_to_try]
yuequ_to_try = peft_to_try + delta_to_try

# %% ../../nbs/02_auto_experiment.ipynb 18
# full_finetune 和 新方法单列
# 这里只跑baseline
from boguan_yuequ.benchmarking import pe_list_tiny_for_all_size, backbone_names
import optuna

def objective(trial, num_of_repeated_experiments = 5):
    # TODO 对每一个目标超参数 分开去做？ grid search
    # for yuequ in yuequ_tried_algs:
    
    meta_parameters = fixed_meta_parameters.copy()
    # 修改超参
    
    # 目标超参被建议
    meta_parameters.yuequ = trial.suggest_categorical("yuequ", yuequ_to_try)
    # choice = trial.suggest_categorical("pe_and_backbone_choice", list(range(len(backbone_names))))
    backbone_name = trial.suggest_categorical("backbone", backbone_names)
    meta_parameters.cls_model_config.checkpoint = backbone_name
    meta_parameters.yuequ_pe = backbone_name2pe[backbone_name]
    trial.set_user_attr("parameter_efficiency", meta_parameters.yuequ_pe) # 用于后续分析实验结果
    
    # 接下来是无关变量
# f"{yuequ}-learning_rate"

    meta_parameters.learning_rate = trial.suggest_float(f"learning_rate", 1e-5, 1e-1, log=True)
    
    result_dict = dict()

    metric_names = set()
    # 重复试验
    for experiment_index in range(num_of_repeated_experiments):
        meta_parameters.experiment_index = experiment_index
        # 当我们选定 experiment_index 之后，就不要随机建议参数了，现在我们元参数保持一样，重复5次随机实验。

        val_result, test_result = run_with_config(
            meta_parameters, trial, "val_acc1", "max"
        )
        # 注意不要用 test_acc1 调参。
        # 我们的原则是每一个目标超参验证集到最优, 然后再用最优的超参得到的模型(其实应该重新训练一遍)在测试集上测试。
        # 在论文研究的第一阶段，应该调参。时间不够的话
        
        single_run_result = val_result[0] | test_result[0]
        metric_names.update(single_run_result.keys())
        single_run_result = {f"{k}-run{experiment_index}":v for k, v in single_run_result.items()}
        for k, v in single_run_result.items():
            trial.set_user_attr(k, v)
        result_dict|=single_run_result
        
        trial.report(single_run_result[f"val_acc1-run{experiment_index}"], experiment_index)
        if trial.should_prune():
            # Return the current predicted value instead of raising `TrialPruned`.
            # This is a workaround to tell the Optuna about the evaluation
            # results in pruned trials.
            for metric_name in metric_names:
                all_runs_results = [result_dict[f"{metric_name}-run{i}"] for i in range(num_of_repeated_experiments)]
                result_dict[f"{metric_name}-mean"] = sum(all_runs_results) / len(all_runs_results)
                trial.set_user_attr(f"{metric_name}-mean", result_dict[f"{metric_name}-mean"])
            trial.set_user_attr(f"num_of_repeated", experiment_index+1)
            return result_dict["val_acc1-mean"]
    # 计算一下平均数
    for metric_name in metric_names:
        all_runs_results = [result_dict[f"{metric_name}-run{i}"] for i in range(num_of_repeated_experiments)]
        result_dict[f"{metric_name}-mean"] = sum(all_runs_results) / len(all_runs_results)
        trial.set_user_attr(f"{metric_name}-mean", result_dict[f"{metric_name}-mean"])
    trial.set_user_attr(f"num_of_repeated", num_of_repeated_experiments)
    return result_dict["val_acc1-mean"]
    

# %% ../../nbs/02_auto_experiment.ipynb 19
from ..utils import runs_path
from optuna.samplers import *
from optuna.pruners import *
# study_path = auto_exp_runs_path / "optuna_studies.db"
# sqlite_url = f"sqlite:///{study_path}"
# sqlite_url = f"sqlite://{study_path}"
# TODO 
postgres_url = 'postgresql+psycopg2://ycm:password@localhost:5432/namable_classify'
# pip install psycopg2-binary 
# postgres_url = 'postgresql://myuser:mypassword@localhost/mydatabase'
# TODO safety
study = optuna.create_study(
    # study_name="peft baselines benchmark",  # old version
    study_name="peft baselines benchmark 11.3", 
    # storage=sqlite_url, 
    storage=postgres_url, 
    load_if_exists=True, 
    direction="maximize", 
    # https://pub.aimind.so/a-deep-dive-in-optunas-advance-features-2e495e71435c
    # sampler=GPSampler(seed=42), 
    # sampler=TPESampler(seed=42), 
    # sampler=TPESampler(), 
    sampler=CmaEsSampler(), 
    # pruner=HyperbandPruner()
    pruner=WilcoxonPruner()
    # CmaEsSampler(seed=42),  我们实验数量应该小于1000
    # WilcoxonPruner(min_n_trials=10) # 不适合这个，这个 immediate 是fold的情况
)
study.set_user_attr("contributors", ["Ye Canming"])
study.set_user_attr("fixed_meta_parameters", fixed_meta_parameters.json())
# 晚点再看
# https://optuna-integration.readthedocs.io/en/stable/reference/generated/optuna_integration.MLflowCallback.html

# %% ../../nbs/02_auto_experiment.ipynb 20
study.optimize(objective, n_trials=100)
