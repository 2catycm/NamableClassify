"""automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_auto_experiment.ipynb.

# %% auto 0
__all__ = ['fixed_meta_parameters', 'start', 'end', 'learning_rates', 'seed', 'run_names', 'run_with_config',
           'learning_rate_exec', 'objective']

# %% ../../nbs/02_auto_experiment.ipynb 4
from ..core import ClassificationTask, ClassificationTaskConfig
from boguan_yuequ.auto import AutoYueQuAlgorithm
import lightning as L
from ..utils import runs_path
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder
from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger

# %% ../../nbs/02_auto_experiment.ipynb 6
def run_with_config(config:ClassificationTaskConfig):
    cls_task = ClassificationTask(config)
    cls_task.print_model_pretty()
    AutoYueQuAlgorithm(cls_task, config.yuequ)
    trainer = L.Trainer(default_root_dir=runs_path, enable_checkpointing=True, 
                    enable_model_summary=True, 
                    num_sanity_val_steps=2, # 防止 val 在训了好久train才发现崩溃
                    callbacks=[
                        # EarlyStopping(monitor="val_loss", mode="min")
                        EarlyStopping(monitor="val_acc1", mode="max", check_finite=True, 
                                    #   patience=5, 
                                      patience=10, 
                                    #   patience=6, 
                                      check_on_train_epoch_end=False,  # check on validation end
                                      verbose=True),
                        ModelSummary(max_depth=3),
                        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/
                        # StochasticWeightAveraging(swa_lrs=1e-2), 
                        # DeviceStatsMonitor(cpu_stats=True)
                        LearningRateMonitor(), 
                        # LearningRateFinder() # 有奇怪的bug
                               ]
                    # , max_epochs=15
                    # , gradient_clip_val=1.0, gradient_clip_algorithm="value"
                    , logger=[
                        # TensorBoardLogger(save_dir=runs_path/"tensorboard"),
                        TensorBoardLogger(save_dir=runs_path),
                              CSVLogger(save_dir=runs_path), 
                              WandbLogger(project="namable_classify", name="test")
                              ]
                    # , profiler="simple"
                    # , fast_dev_run=True
                    # limit_train_batches=10, limit_val_batches=5
                    # strategy="ddp", accelerator="gpu", devices=4
                    )
    trainer.fit(cls_task, datamodule=cls_task.lit_data)
    val_result = trainer.validate(cls_task, datamodule=cls_task.lit_data)
    test_result = trainer.test(cls_task, datamodule=cls_task.lit_data)
    # val_acc1 = val_result[0]["val_acc1"]
    # test_acc1 = test_result[0]["test_acc1"]
    # return val_acc1, test_acc1
    return val_result, test_result
    

# %% ../../nbs/02_auto_experiment.ipynb 7
from ..core import ClassificationModelConfig, ClassificationTaskConfig, ClassificationDataConfig
fixed_meta_parameters = ClassificationTaskConfig(
    label_smoothing=0.1,  # 未必固定。
    cls_model_config=ClassificationModelConfig(
        checkpoint = "google/vit-base-patch16-224-in21k"
    ), 
    dataset_config = ClassificationDataConfig(
        batch_size=64, # 经过前期经验, 这个方便站在61服务器跑, 大概10G显存。 固定基于这个调参
    )
)


# %% ../../nbs/02_auto_experiment.ipynb 8
# 先直接跑两个, 来不及写了

import numpy as np
# 设置采样的起始值和结束值
start = np.log(1e-5)
end = np.log(1e-1)
learning_rates = np.logspace(start, end, num=30, base=np.e)
np.random.shuffle(learning_rates)
learning_rates = learning_rates.tolist()

# %% ../../nbs/02_auto_experiment.ipynb 10
# seed = 0
seed = 2
# seed = 1
def learning_rate_exec(learning_rate):
    parameters = fixed_meta_parameters.copy()
    parameters.yuequ = 'full_finetune'
    parameters.experiment_index = seed
    parameters.learning_rate = learning_rate
    return run_with_config(parameters)

# %% ../../nbs/02_auto_experiment.ipynb 11
run_names = [f"{lr:.2e}" for lr in learning_rates]

# %% ../../nbs/02_auto_experiment.ipynb 13
#| eval: false
from .run import auto_run
auto_run(learning_rate_exec, learning_rates, run_names, f"sweep_lr_full_finetune-{seed}")

# %% ../../nbs/02_auto_experiment.ipynb 14
import optuna
def objective(trial):
    
    # parameter_efficiency_budget = trial.suggest_float("parameter_efficiency_budget", 1e-7, 1, log=True)
    # 对每一个目标超参数 grid search
    result_dict = dict()
    for yuequ in ["full_finetune", "adapter", "LORA"]:
        for experiment_index in range(5):
            # 每一个人的hyperparameters不一样。
            learning_rate = trial.suggest_float(f"{yuequ}-learning_rate", 1e-5, 1e-1, log=True)
            config = ClassificationTaskConfig(
                yuequ=yuequ,
                experiment_index=experiment_index,
                learning_rate=learning_rate,
            )
            val_acc1, test_acc1 = run_with_config(config)
            # 注意不要用 test_acc1 调参。
            # 我们的原则是每一个目标超参验证集到最优, 然后再用最优的超参得到的模型(其实应该重新训练一遍)在测试集上测试。
            # 在论文研究的第一阶段，应该调参。时间不够的话
            result_dict[f"{yuequ}-{experiment_index}-val_acc1"] = val_acc1
            result_dict[f"{yuequ}-{experiment_index}-test_acc1"] = test_acc1
        mean_results = [result_dict[f"{yuequ}-{i}-val_acc1"] for i in range(5)]
        result_dict[f"{yuequ}-mean-val_acc1"] = sum(mean_results) / len(mean_results)
        mean_results = [result_dict[f"{yuequ}-{i}-test_acc1"] for i in range(5)]
        result_dict[f"{yuequ}-mean-test_acc1"] = sum(mean_results) / len(mean_results)
    return result_dict
    
