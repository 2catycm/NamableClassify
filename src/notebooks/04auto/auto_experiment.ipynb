{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto_experiment\n",
    "> automatically research on the relationship between the performance and meta parameters (a.k.a. hyperparameters or config) via searching (a.k.a. sweeping) experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/google-research/tuning_playbook for scientific research principles on meta parameters tuning. \n",
    "\n",
    "In addition to that guide, we also follow the paper \"Statistical Comparisons of Classifiers over Multiple Data Sets\", using statistical hypothesis testing to compare the performance of different models (produced by different meta parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp auto.experiment.infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-30 22:52:25.141246</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-30 22:52:25.141246\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Note: NumExpr detected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48</span> cores but <span style=\"color: #008000; text-decoration-color: #008000\">\"NUMEXPR_MAX_THREADS\"</span> not set, so enforcing safe limit of <a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#148\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148</span></a>\n",
       "         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>.                                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Note: NumExpr detected \u001b[1;36m48\u001b[0m cores but \u001b[32m\"NUMEXPR_MAX_THREADS\"\u001b[0m not set, so enforcing safe limit of \u001b]8;id=586869;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=949028;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#148\u001b\\\u001b[2m148\u001b[0m\u001b]8;;\u001b\\\n",
       "         \u001b[1;36m8\u001b[0m.                                                                                            \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-30 22:52:25.153487</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-30 22:52:25.153487\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> NumExpr defaulting to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> threads.                                                              <a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#160\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">160</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m NumExpr defaulting to \u001b[1;36m8\u001b[0m threads.                                                              \u001b]8;id=851876;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=556894;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#160\u001b\\\u001b[2m160\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from namable_classify.infra import runs_path\n",
    "from namable_classify.nucleus import ClassificationTask, ClassificationTaskConfig\n",
    "from boguan_yuequ.auto.nucleus import AutoYueQuAlgorithm\n",
    "import lightning as L\n",
    "from namable_classify.infra import runs_path\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelSummary, StochasticWeightAveraging, DeviceStatsMonitor, LearningRateMonitor, LearningRateFinder, BatchSizeFinder\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger, WandbLogger\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from namable_classify.infra import logger\n",
    "import torch\n",
    "# from clearml import Task\n",
    "from lightning.pytorch.profilers import AdvancedProfiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OneBitAdam' from 'deepspeed.ops.adam' (/home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/deepspeed/ops/adam/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningRateFinder\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# LearningRateFinder?\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepSpeedCPUAdam, FusedAdam, OneBitAdam\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OneBitAdam' from 'deepspeed.ops.adam' (/home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/deepspeed/ops/adam/__init__.py)"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import LearningRateFinder\n",
    "# LearningRateFinder?\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n",
    "from deepspeed.ops.lamb import FusedLamb\n",
    "# DeepSpeedCPUAdam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# 重要\n",
    "# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399\n",
    "\n",
    "\n",
    "auto_exp_runs_path = runs_path / \"auto_experiment\"\n",
    "auto_exp_runs_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "checkpoint_path = \"/ccfa/ycm/namable_classify\"\n",
    "\n",
    "\n",
    "\n",
    "def run_with_config(\n",
    "    config: ClassificationTaskConfig,\n",
    "    trial: optuna.Trial = None,\n",
    "    tuning_metric=\"val_acc1\",  # Seriously, 为了学术诚信规范，我们AI科研者不能用 \"test_acc1\" 来调参。\n",
    "    tuning_mode=\"max\", \n",
    "    batch_size=-1,\n",
    "    need_reproduce=False, # 调参阶段不需要复现，需要快速迭代\n",
    "    running_path:Path = auto_exp_runs_path,\n",
    "    checkpoint_path:str = checkpoint_path,\n",
    "    search_learning_rate = False,\n",
    "    # fast_dev_run=False, # 行为不太一样\n",
    "    profile = False,\n",
    "    limit_train_batches = None,\n",
    "    limit_val_batches = None,\n",
    "    limit_test_batches = None,\n",
    "    max_epochs = 30,\n",
    "    **kwargs\n",
    "):\n",
    "    logger.info(f\"running with config: {config}\")\n",
    "    if need_reproduce:\n",
    "        L.seed_everything(config.experiment_index)\n",
    "    cls_task = ClassificationTask(config)\n",
    "    # cls_task.print_model_pretty()\n",
    "    # AutoYueQuAlgorithm(cls_task, config.yuequ, config.yuequ_pe)\n",
    "    cls_task.cls_model.backbone = AutoYueQuAlgorithm(cls_task.cls_model.backbone,\n",
    "        config.yuequ, config.yuequ_pe).adapted_module\n",
    "    cls_task.print_trainable_parameters()\n",
    "    \n",
    "    # AutoYueQuAlgorithm(cls_task.cls_model, config.yuequ, config.yuequ_pe)\n",
    "    # Task.init(project_name=config.experiment_project, task_name=config.experiment_task)\n",
    "    # https://clear.ml/docs/latest/docs/guides/frameworks/pytorch_lightning/pytorch_lightning_example/\n",
    "    best_checkpoint_callback = ModelCheckpoint(\n",
    "            save_top_k=1, # 只保存一个最好的模型\n",
    "            monitor=tuning_metric,\n",
    "            mode=tuning_mode,\n",
    "            dirpath=checkpoint_path,\n",
    "            filename=f'{config.yuequ}-{config.dataset_config.dataset_name}'+'-{epoch:02d}-{' + tuning_metric + ':.2f}',\n",
    "        )\n",
    "    callbacks = [\n",
    "        best_checkpoint_callback, \n",
    "        # ModelCheckpoint(\n",
    "        #     save_top_k=1, # 只保存最后一个\n",
    "        #     # monitor=\"global_step\",\n",
    "        #     monitor=\"epoch\",\n",
    "        #     mode=\"max\",\n",
    "        #     dirpath=checkpoint_path,\n",
    "        #     save_last = 'link', # 软链接过去，其实只保存一个就是那个\n",
    "        #     filename=f'{config.yuequ}-{config.dataset_config.dataset_name}'+'-{epoch:02d}',\n",
    "        # ),  \n",
    "        EarlyStopping(\n",
    "            monitor=tuning_metric,\n",
    "            mode=tuning_mode,\n",
    "            check_finite=True,\n",
    "            #   patience=5,\n",
    "            patience=10,\n",
    "            #   patience=6,\n",
    "            check_on_train_epoch_end=False,  # check on validation end\n",
    "            verbose=True,\n",
    "        ),\n",
    "        ModelSummary(max_depth=3),\n",
    "        # https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n",
    "        # StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "        # DeviceStatsMonitor(cpu_stats=True)\n",
    "        # LearningRateMonitor(),\n",
    "        # LearningRateFinder() # 有奇怪的bug\n",
    "        # BatchSizeFinder(init_val=32) # 用 \"power\" 减少调参不确定性; \n",
    "    ]\n",
    "    if trial is not None:\n",
    "        callbacks.append(PyTorchLightningPruningCallback(trial, monitor=tuning_metric))\n",
    "\n",
    "    lightning_loggers = [\n",
    "        TensorBoardLogger(save_dir=running_path, log_graph=True),\n",
    "        CSVLogger(save_dir=running_path),\n",
    "        # MLFlowLogger(experiment_name=f\"{config.experiment_project}/{config.experiment_task}\", \n",
    "        #             #  .replace(\"_\", \"-\").replace(\" \", \"-\"), \n",
    "        #              tracking_uri=\"http://10.103.10.55:5000\")\n",
    "        # WandbLogger(project=config.experiment_project, name=config.experiment_task),\n",
    "    ]\n",
    "    \n",
    "    torch.set_float32_matmul_precision(\"high\") # 这个只是对矩阵乘法生效，只是为了加速\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=running_path,\n",
    "        # enable_checkpointing=True,\n",
    "        enable_model_summary=True,\n",
    "        num_sanity_val_steps=2,  # 防止 val 在训了好久train才发现崩溃\n",
    "        callbacks=callbacks\n",
    "        , max_epochs=max_epochs \n",
    "        # , gradient_clip_val=1.0, gradient_clip_algorithm=\"value\"\n",
    "        ,\n",
    "        logger=lightning_loggers,\n",
    "        # , profiler=\"simple\"\n",
    "        # , fast_dev_run=True\n",
    "        # limit_train_batches=10, limit_val_batches=5\n",
    "        # strategy=\"ddp\", accelerator=\"gpu\", devices=4\n",
    "        accelerator=\"gpu\", devices=1, # 实验并行优于数据并行，尽量单卡训练\n",
    "        # accelerator=\"gpu\", devices=8, \n",
    "        # precision=16 \n",
    "        # accelerator=\"gpu\", devices=2, strategy=\"deepspeed_stage_2_offload\", precision=\"16-mixed\",\n",
    "        # strategy=\"ddp\"\n",
    "        benchmark=not need_reproduce,\n",
    "        deterministic=need_reproduce,\n",
    "        # profiler = AdvancedProfiler(dirpath=\".\", \n",
    "        #                             filename=(running_path/\"profiler_logs.txt\").as_posix(), # 必须str\n",
    "        #                             ) # speed check\n",
    "        profiler='simple' if profile else None,\n",
    "        # fast_dev_run = fast_dev_run, \n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        limit_test_batches=limit_test_batches,\n",
    "        **kwargs\n",
    "    )\n",
    "    trainer_log_dir_path = Path(trainer.log_dir)\n",
    "    trainer_log_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    # if profile:\n",
    "    #     profiler = AdvancedProfiler(dirpath=trainer.log_dir, \n",
    "    #                                 filename=\"perf_logs\"), \n",
    "    #     trainer.profiler = profiler\n",
    "    \n",
    "    # batch size 和 learning rate\n",
    "    if batch_size == -1 or search_learning_rate:\n",
    "        from lightning.pytorch.tuner import Tuner\n",
    "        tuner_trainer = L.Trainer()\n",
    "        tuner = Tuner(tuner_trainer)\n",
    "    if batch_size == -1:\n",
    "        # Batch size finder is not yet supported for DDP or any of its variations, it is coming soon.\n",
    "        \n",
    "        found_batch_size = tuner.scale_batch_size(cls_task, datamodule=cls_task.lit_data, \n",
    "                                            #   mode='binsearch', \n",
    "                                            mode='power', \n",
    "                                            init_val=config.dataset_config.batch_size)\n",
    "    else:\n",
    "        found_batch_size = batch_size\n",
    "    cls_task.hparams.batch_size = found_batch_size\n",
    "    \n",
    "    # config.dataset_config.batch_size = found_batch_size\n",
    "    # cls_task = ClassificationTask(config) # 防止状态错误。\n",
    "    # cls_task.hparams.batch_size = found_batch_size\n",
    "    \n",
    "    # https://github.com/davidtvs/pytorch-lr-finder \n",
    "    # 更加精细控制\n",
    "    if search_learning_rate:\n",
    "        \n",
    "        lr_finder = tuner.lr_find(cls_task, datamodule=cls_task.lit_data, \n",
    "                                  max_lr=1e-2, min_lr=1e-7, \n",
    "                                  num_training=100)\n",
    "        print(lr_finder.results)\n",
    "        fig = lr_finder.plot(suggest=True)\n",
    "        # fig.show()\n",
    "        plt.savefig(trainer_log_dir_path/\"lr_finder.png\")\n",
    "        lightning_loggers[0].experiment.add_figure(\"lr_finder\", fig, global_step=0) # tensorboard\n",
    "        new_lr = lr_finder.suggestion()\n",
    "        \n",
    "        cls_task.hparams.learning_rate = new_lr\n",
    "        \n",
    "        # config.learning_rate = new_lr\n",
    "        # cls_task = ClassificationTask(config) # 防止状态错误\n",
    "    else:\n",
    "        linear_lr_scale = found_batch_size / 64\n",
    "        cls_task.hparams.learning_rate = config.learning_rate * linear_lr_scale\n",
    "        logger.info(f\"original learning rate: {config.learning_rate}, linear lr scale: {linear_lr_scale}, learning rate: {cls_task.hparams.learning_rate}\")\n",
    "\n",
    "    \n",
    "\n",
    "    logger.info(f\"actual hyperparameters: {cls_task.hparams}\")\n",
    "    \n",
    "    trainer.fit(cls_task, datamodule=cls_task.lit_data)\n",
    "    \n",
    "    # 重新计算 Early Stop的时候，最好的那一个模型\n",
    "    # best_cls_task_model = cls_task.load_from_checkpoint(best_checkpoint_callback.best_model_path)\n",
    "    # val_result = trainer.validate(best_cls_task_model, datamodule=cls_task.lit_data)\n",
    "    # test_result = trainer.test(best_cls_task_model, datamodule=cls_task.lit_data)\n",
    "    \n",
    "    val_result = trainer.validate(ckpt_path='best', datamodule=cls_task.lit_data)\n",
    "    test_result = trainer.test(ckpt_path='best', datamodule=cls_task.lit_data)\n",
    "    \n",
    "    results = val_result[0] | test_result[0]\n",
    "    \n",
    "    results_json_path = trainer_log_dir_path/\"results.json\"\n",
    "    with open(results_json_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "        \n",
    "    # val_acc1 = val_result[0][\"val_acc1\"]\n",
    "    # test_acc1 = test_result[0][\"test_acc1\"]\n",
    "    # return val_acc1, test_acc1\n",
    "    return val_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from namable_classify.nucleus import ClassificationModelConfig, ClassificationTaskConfig, ClassificationDataConfig\n",
    "fixed_meta_parameters = ClassificationTaskConfig(\n",
    "    experiment_project = \"Homogeneous dwarf model is all you need for tuning pretrained giant model.\", \n",
    "    # experiment_name = \"Auto experiment\", \n",
    "    experiment_task = \"Auto experiment Stage 1 (single run, short epoches)\", \n",
    "    label_smoothing=0.1,  # 未必固定。\n",
    "    cls_model_config=ClassificationModelConfig(\n",
    "        # checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "    ), \n",
    "    dataset_config = ClassificationDataConfig(\n",
    "        # batch_size=64, # 经过前期经验, 这个方便站在61服务器跑, 大概10G显存。 固定基于这个调参\n",
    "        batch_size=16,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "study_path = auto_exp_runs_path / \"optuna_studies.db\"\n",
    "sqlite_url = f\"sqlite:///{study_path}\"\n",
    "# sqlite_url = f\"sqlite://{study_path}\"\n",
    "\n",
    "# pip install psycopg2-binary \n",
    "\n",
    "@dataclass\n",
    "class PostgresDatabaseConfig:\n",
    "    username: str\n",
    "    password: str\n",
    "    host: str\n",
    "    port: int\n",
    "    database_name: str\n",
    "    postgres_protocol: str = 'postgresql+psycopg2'\n",
    "\n",
    "    @property\n",
    "    def sqlalchemy_url(self) -> str:\n",
    "#    postgres_url = 'postgresql://myuser:mypassword@localhost/mydatabase'\n",
    "    \n",
    "        return f'{self.postgres_protocol}://{self.username}:{self.password}@{self.host}:{self.port}/{self.database_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import json\n",
    "from namable_classify.infra import runs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "database_config_path = runs_path / 'database_config.json'\n",
    "with open(database_config_path, 'r') as f:\n",
    "    config = PostgresDatabaseConfig(**json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(config.sqlalchemy_url)\n",
    "# engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
