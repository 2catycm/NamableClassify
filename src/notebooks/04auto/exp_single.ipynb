{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto_experiment for a single method\n",
    "> 为单个方法自动实验，优化掉冗余超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp auto.experiment.single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-30 22:32:23.396866</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-30 22:32:23.396866\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Note: NumExpr detected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48</span> cores but <span style=\"color: #008000; text-decoration-color: #008000\">\"NUMEXPR_MAX_THREADS\"</span> not set, so enforcing safe limit of <a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#148\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148</span></a>\n",
       "         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>.                                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Note: NumExpr detected \u001b[1;36m48\u001b[0m cores but \u001b[32m\"NUMEXPR_MAX_THREADS\"\u001b[0m not set, so enforcing safe limit of \u001b]8;id=557593;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=181817;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#148\u001b\\\u001b[2m148\u001b[0m\u001b]8;;\u001b\\\n",
       "         \u001b[1;36m8\u001b[0m.                                                                                            \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">Sat 2024-11-30 22:32:23.410601</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36mSat 2024-11-30 22:32:23.410601\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> NumExpr defaulting to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> threads.                                                              <a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#160\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">160</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m NumExpr defaulting to \u001b[1;36m8\u001b[0m threads.                                                              \u001b]8;id=952377;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=339364;file:///home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/numexpr/utils.py#160\u001b\\\u001b[2m160\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/ycm/program_files/miniconda3/envs/fastai/lib/python3.10/site-packages/optuna/samplers/_cmaes.py:292: ExperimentalWarning: `consider_pruned_trials` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2024-11-30 22:32:41,036] Using an existing study with name 'peft baselines benchmark 11.29' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from namable_classify.auto.experiment.infra import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "study_results = [] # 准备装入 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# 需要跑哪些backbone 和 对应的 pe呢？\n",
    "from boguan_yuequ.benchmarking import pe_list_tiny_for_all_size, backbone_names\n",
    "# from transformers import AutoModel\n",
    "# # tiny+tiny vs tiny full vs tiny full_lora 也是有意义的对比，所以不做截断。\n",
    "# for config, pe in zip(configs, pe_list_tiny_for_all_size):\n",
    "#     model = AutoModel.from_pretrained(config)\n",
    "#     yuequ = AutoYueQuAlgorithm(model, 'lora', pe)\n",
    "#     model = yuequ.adapted_model\n",
    "#     pe = yuequ.pe\n",
    "backbone_name2pe = {backbone_name:pe for pe, backbone_name in zip(pe_list_tiny_for_all_size, backbone_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'WinKawaks/vit-tiny-patch16-224'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-small'\u001b[0m: \u001b[1;36m0.2525765133213994\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-base'\u001b[0m: \u001b[1;36m0.06439934417512297\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-large'\u001b[0m: \u001b[1;36m0.018329073267526527\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-giant'\u001b[0m: \u001b[1;36m0.004909821625242288\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_name2pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# 需要跑哪些算法呢？\n",
    "# from boguan_yuequ.auto import huggingface_peft_budget_config_key, thu_nlp_opendelta_budget_config_key\n",
    "from boguan_yuequ.auto.integrations.peft import huggingface_peft_budget_config_key\n",
    "from boguan_yuequ.auto.integrations.opendelta import thunlp_opendelta_budget_config_key\n",
    "\n",
    "peft_to_try = [k.name for k in huggingface_peft_budget_config_key.keys()]\n",
    "delta_to_try = [k for k in thunlp_opendelta_budget_config_key.keys() if k.upper() not in peft_to_try]\n",
    "yuequ_to_try = peft_to_try + delta_to_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'LORA'\u001b[0m,\n",
       "    \u001b[32m'ADALORA'\u001b[0m,\n",
       "    \u001b[32m'LOHA'\u001b[0m,\n",
       "    \u001b[32m'OFT'\u001b[0m,\n",
       "    \u001b[32m'VERA'\u001b[0m,\n",
       "    \u001b[32m'FOURIERFT'\u001b[0m,\n",
       "    \u001b[32m'VBLORA'\u001b[0m,\n",
       "    \u001b[32m'adapter'\u001b[0m,\n",
       "    \u001b[32m'delta_yuequ_prefix_tuning'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yuequ_to_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:32:41,648] A new study created in RDB with name: peft baselines benchmark 11.30 full_finetune giant\n"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "import optuna\n",
    "\n",
    "from optuna.samplers import *\n",
    "from optuna.pruners import *\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    # study_name=\"peft baselines benchmark\",  # old version\n",
    "    # study_name=\"peft baselines benchmark 11.3\", \n",
    "    # study_name=\"peft baselines benchmark 11.7\", \n",
    "    # study_name=\"peft baselines benchmark 11.30 wave_high_ladder\", \n",
    "    # study_name=\"peft baselines benchmark 11.30 full_finetune\", \n",
    "    study_name=\"peft baselines benchmark 11.30 full_finetune giant\", \n",
    "    # storage=config.sqlalchemy_url, \n",
    "    storage=sqlite_url, \n",
    "    load_if_exists=True, \n",
    "    direction=\"maximize\", \n",
    "    # https://pub.aimind.so/a-deep-dive-in-optunas-advance-features-2e495e71435c\n",
    "    # sampler=GPSampler(seed=42), \n",
    "    # sampler=TPESampler(seed=42), \n",
    "    # sampler=TPESampler(), \n",
    "    # https://github.com/optuna/optuna/issues/1647\n",
    "    sampler=CmaEsSampler(consider_pruned_trials = True), \n",
    "    pruner=HyperbandPruner()\n",
    "    # pruner=WilcoxonPruner()\n",
    "    # CmaEsSampler(seed=42),  我们实验数量应该小于1000\n",
    "    # WilcoxonPruner(min_n_trials=10) # 不适合这个，这个 immediate 是fold的情况\n",
    ")\n",
    "study.set_user_attr(\"contributors\", [\"Ye Canming\"])\n",
    "study.set_user_attr(\"fixed_meta_parameters\", fixed_meta_parameters.json())\n",
    "# 晚点再看\n",
    "# https://optuna-integration.readthedocs.io/en/stable/reference/generated/optuna_integration.MLflowCallback.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'WinKawaks/vit-tiny-patch16-224'\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-small'\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-base'\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-large'\u001b[0m,\n",
       "    \u001b[32m'facebook/dinov2-giant'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# full_finetune 和 新方法单列\n",
    "# 这里只跑baseline\n",
    "from boguan_yuequ.benchmarking import pe_list_tiny_for_all_size, backbone_names\n",
    "import optuna\n",
    "from namable_classify.infra import logger\n",
    "from rich.prompt import Prompt\n",
    "\n",
    "import optuna.exceptions\n",
    "\n",
    "def objective(trial, num_of_repeated_experiments = 5):\n",
    "    # TODO 对每一个目标超参数 分开去做？ grid search\n",
    "    # for yuequ in yuequ_tried_algs:\n",
    "    \n",
    "    meta_parameters = fixed_meta_parameters.copy()\n",
    "    # 修改超参\n",
    "    \n",
    "    # 目标超参被建议\n",
    "    # meta_parameters.yuequ = trial.suggest_categorical(\"yuequ\", yuequ_to_try)\n",
    "    # meta_parameters.yuequ = trial.suggest_categorical(\"yuequ\", [\"wave_high_ladder\"])\n",
    "    meta_parameters.yuequ = trial.suggest_categorical(\"yuequ\", [\"full_finetune\"])\n",
    "    # choice = trial.suggest_categorical(\"pe_and_backbone_choice\", list(range(len(backbone_names))))\n",
    "    # backbone_name = trial.suggest_categorical(\"backbone\", backbone_names)\n",
    "    backbone_name = trial.suggest_categorical(\"backbone\", [backbone_names[-1]])\n",
    "    \n",
    "    meta_parameters.cls_model_config.checkpoint = backbone_name\n",
    "    meta_parameters.yuequ_pe = backbone_name2pe[backbone_name]\n",
    "    trial.set_user_attr(\"parameter_efficiency\", meta_parameters.yuequ_pe) # 用于后续分析实验结果\n",
    "    \n",
    "    # 接下来是无关变量\n",
    "# f\"{yuequ}-learning_rate\"\n",
    "\n",
    "    # meta_parameters.learning_rate = trial.suggest_float(f\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    # meta_parameters.learning_rate = trial.suggest_float(f\"learning_rate\", 1e-5, 1e-2, log=True) # 正则化，建议的是Batch Size=64的学习率\n",
    "    meta_parameters.learning_rate = trial.suggest_float(f\"learning_rate\", 1e-4, 4e-2, log=True) # 正则化，建议的是Batch Size=64的学习率\n",
    "    \n",
    "    result_dict = dict()\n",
    "\n",
    "    metric_names = set()\n",
    "    # 重复试验\n",
    "    for experiment_index in range(num_of_repeated_experiments):\n",
    "        meta_parameters.experiment_index = experiment_index\n",
    "        # 当我们选定 experiment_index 之后，就不要随机建议参数了，现在我们元参数保持一样，重复5次随机实验。\n",
    "        try:\n",
    "            val_result, test_result = run_with_config(\n",
    "                meta_parameters, trial, \"val_acc1\", \"max\", \n",
    "                batch_size=1, \n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            logger.error(f\"Error in experiment {experiment_index}, May be you can\\n\"\n",
    "                         \"1. Stop the optuna study and you debug and fix the buggy code. \"\n",
    "                         \"2. Searched optuna trial is invalid as an input, so just prune this trial and continue. \"\n",
    "                         )\n",
    "            choice = Prompt.ask(\"What should we do now?\",\n",
    "                                choices=[\"1\", \"2\"], default=\"1\")\n",
    "            if choice == \"1\":\n",
    "                raise e\n",
    "            else:\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "        # 注意不要用 test_acc1 调参。\n",
    "        # 我们的原则是每一个目标超参验证集到最优, 然后再用最优的超参得到的模型(其实应该重新训练一遍)在测试集上测试。\n",
    "        # 在论文研究的第一阶段，应该调参。时间不够的话\n",
    "        \n",
    "        single_run_result = val_result[0] | test_result[0]\n",
    "        metric_names.update(single_run_result.keys())\n",
    "        single_run_result = {f\"{k}-run{experiment_index}\":v for k, v in single_run_result.items()}\n",
    "        for k, v in single_run_result.items():\n",
    "            trial.set_user_attr(k, v)\n",
    "        result_dict|=single_run_result\n",
    "        \n",
    "        trial.report(single_run_result[f\"val_acc1-run{experiment_index}\"], experiment_index)\n",
    "        # if trial.should_prune():\n",
    "        #     # Return the current predicted value instead of raising `TrialPruned`.\n",
    "        #     # This is a workaround to tell the Optuna about the evaluation\n",
    "        #     # results in pruned trials.\n",
    "        #     for metric_name in metric_names:\n",
    "        #         all_runs_results = [result_dict[f\"{metric_name}-run{i}\"] for i in range(num_of_repeated_experiments)]\n",
    "        #         result_dict[f\"{metric_name}-mean\"] = sum(all_runs_results) / len(all_runs_results)\n",
    "        #         trial.set_user_attr(f\"{metric_name}-mean\", result_dict[f\"{metric_name}-mean\"])\n",
    "        #     trial.set_user_attr(f\"num_of_repeated\", experiment_index+1)\n",
    "        #     return result_dict[\"val_acc1-mean\"]\n",
    "    # 计算一下平均数\n",
    "    for metric_name in metric_names:\n",
    "        all_runs_results = [result_dict[f\"{metric_name}-run{i}\"] for i in range(num_of_repeated_experiments)]\n",
    "        result_dict[f\"{metric_name}-mean\"] = sum(all_runs_results) / len(all_runs_results)\n",
    "        trial.set_user_attr(f\"{metric_name}-mean\", result_dict[f\"{metric_name}-mean\"])\n",
    "    trial.set_user_attr(f\"num_of_repeated\", num_of_repeated_experiments)\n",
    "    return result_dict[\"val_acc1-mean\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# study.optimize(objective, n_trials=100)\n",
    "study.optimize(lambda trial: objective(trial, num_of_repeated_experiments=1), \n",
    "               n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 05:34:34,081] Using an existing study with name 'peft baselines benchmark 11.29' instead of creating a new one.\n",
      "[autoreload of namable_classify.data failed: Traceback (most recent call last):\n",
      "  File \"/home/ye_canming/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ye_canming/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/ye_canming/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/ye_canming/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/ye_canming/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 319, in update_instances\n",
      "    refs = gc.get_referrers(old)\n",
      "KeyboardInterrupt\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
